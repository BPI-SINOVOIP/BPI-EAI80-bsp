/**
 *
 * Copyright (C) 2020 Gree Edgeless Microelectronics. All Rights Reserved.
 *
 * @file        jpeg.c
 *
 * @author      Wen.Liu
 *
 * @version     v1.0.0
 *
 * @date        2020/03/16
 *
 * @brief       Speed up jpeg by kdp
 *
 * @note
 *              2020/03/16, Wen.Liu, v1.0.0
 *                  Initial version.
 *
 **/

#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "image.h"
#include "npu/npu.h"
#include "jpeg.h"

#define USE_SRAM_OPT
#define SRAM_OPT_START_ADDR 0x20000000

#define ARMC4_OPT
//#define ASM_PUT_BLOCK
#define ASM_PUT_BLOCK_SHORT_LOOP

#define MAX_CONV_CMD_SIZE    100
#define MAX_WEIGHT_SIZE 1024
#define MAX_DCT_COEF_SIZE 1024
#define MAX_IDCT_COEF_SIZE 1024

/* Fast DCT
t0 = a0 + a7
t1 = a1 + a6
t2 = a2 + a5
t3 = a3 + a4
t7 = a0 - a7
t6 = a1 - a6
t5 = a2 - a5
t4 = a3 - a4

t10 = t0 + t3 = a0 + a7 + a3 + a4
t13 = t0 - t3 = a0 + a7 - a3 - a4
t11 = t1 + t2 = a1 + a6 + a2 + a5
t12 = t1 - t2 = a1 + a6 - a2 - a5
z1  = 0.71(t12+ t13)= 0.71*a0 + 0.71*a7 - 0.71*a3 - 0.71*a4 + 0.71*a1 + 0.71*a6 - 0.71*a2 - 0.71*a5

P0  = t10 + t11 = a0 + a7 + a3 + a4 + a1 + a6 + a2 + a5
P4  = t10 - t11 = a0 + a7 + a3 + a4 - a1 - a6 - a2 - a5
P2  = t13 + z1  = (1+0.71)a0 + (1+0.71)a7 - (1+0.71)a3 - (1+0.71)a4 + (0.71)a1 + (0.71)a6 - (0.71)a2 - (0.71)a5
P6  = t13 - z1  = (1-0.71)a0 + (1-0.71)a7 - (1-0.71)a3 - (1-0.71)a4 - (0.71)a1 - (0.71)a6 + (0.71)a2 + (0.71)a5


t10 = t4 + t5 = a2 + a3 - a4 - a5
t11 = t5 + t6 = a1 + a2 - a5 - a6
t12 = t6 + t7 = a0 + a1 - a6 - a7
z5  = 0.38(t10 - t12) = 0.38*a2 + 0.38*a3 - 0.38*a4 - 0.38*a5 - 0.38*a0 - 0.38*a1 + 0.38*a6 + 0.38*a7
z2  = 0.54*t10 + z5   = (0.54+0.38)a2 + (0.54+0.38)a3 - (0.54+0.38)a4 - (0.54+0.38)a5 - (0.38)a0 - (0.38)a1 + (0.38)a6 + (0.38)a7
z4  = 1.31*t12 + z5   = (0.38)a2 + (0.38)a3 - (0.38)a4 - (0.38)a5 + (1.31-0.38)a0 + (1.31-0.38)a1 - (1.31-0.38)a6 - (1.31-0.38)a7
z3  = 0.71*t11 = 0.71*a1 + 0.71*a2 - 0.71*a5 - 0.71*a6
z11 = t7 + z3 = a0 - a7 + (0.71)a1 + (0.71)a2 - (0.71)a5 - (0.71)a6
z13 = t7 - z3 = a0 - a7 - (0.71)a1 - (0.71)a2 + (0.71)a5 + (0.71)a6

P5  = z13 + z2 = (1-0.38)a0 - (1-0.38)a7 - (0.71 + 0.38)a1 + (0.54+0.38-0.71)a2 - (0.54+0.38-0.71)a5 + (0.71+0.38)a6 + (0.54+0.38)a3 - (0.54+0.38)a4
P3  = z13 - z2 = (1+0.38)a0 - (1+0.38)a7 - (0.71 - 0.38)a1 - (0.54+0.38+0.71)a2 + (0.54+0.38+0.71)a5 + (0.71-0.38)a6 - (0.54+0.38)a3 + (0.54+0.38)a4
P1  = z11 + z4 = (1+1.31-0.38)a0 - (1+1.31-0.38)a7 + (0.71+1.31-0.38)a1 + (0.71+0.38)a2 - (0.71+0.38)a5 - (0.71+1.31-0.38)a6 + (0.38)a3 - (0.38)a4
P7  = z11 - z4 = (1-1.31+0.38)a0 - (1-1.31+0.38)a7 - (1.31-0.71-0.38)a1 + (0.71-0.38)a2 - (0.71-0.38)a5 + (1.31-0.38-0.71)a6 - (0.38)a3 + (0.38)a4

P0  =              a0 +                 a1 +                 a2 +            a3 +            a4 +                 a5 +                 a6 +              a7
P1  = (1+1.31-0.38)a0 + (0.71+1.31-0.38)a1 +      (0.71+0.38)a2 +      (0.38)a3 -      (0.38)a4 -      (0.71+0.38)a5 - (0.71+1.31-0.38)a6 - (1+1.31-0.38)a7
P2  =      (1+0.71)a0 +           (0.71)a1 -           (0.71)a2 -    (1+0.71)a3 -    (1+0.71)a4 -           (0.71)a5 +           (0.71)a6 +      (1+0.71)a7
P3  =      (1+0.38)a0 -    (0.71 - 0.38)a1 - (0.54+0.38+0.71)a2 - (0.54+0.38)a3 + (0.54+0.38)a4 + (0.54+0.38+0.71)a5 +      (0.71-0.38)a6 -      (1+0.38)a7
P4  =              a0 -                 a1 -                 a2 +            a3 +            a4 -                 a5 -                 a6 +              a7
P5  =      (1-0.38)a0 -    (0.71 + 0.38)a1 + (0.54+0.38-0.71)a2 + (0.54+0.38)a3 - (0.54+0.38)a4 - (0.54+0.38-0.71)a5 +      (0.71+0.38)a6 -      (1-0.38)a7
P6  =      (1-0.71)a0 -           (0.71)a1 +           (0.71)a2 -    (1-0.71)a3 -    (1-0.71)a4 +           (0.71)a5 -           (0.71)a6 +      (1-0.71)a7
P7  = (1-1.31+0.38)a0 - (1.31-0.71-0.38)a1 +      (0.71-0.38)a2 -      (0.38)a3 +      (0.38)a4 -      (0.71-0.38)a5 + (1.31-0.38-0.71)a6 - (1-1.31+0.38)a7
*/
#define FIX_1_0000000    ((int32_t)  256)
#define FIX_0_382683433  ((int32_t)   98)
#define FIX_0_541196100  ((int32_t)  139)
#define FIX_0_707106781  ((int32_t)  181)
#define FIX_1_306562965  ((int32_t)  334)

#define DESCALE(x, y)   ((x)>>(y))
#define MULTIPLY(x, y)  ((x) * (y))
#define LMULTIPLY(x, y)  DESCALE((x) * (y), 8)

#define FIX001  ((int32_t)   256)
#define FIX038  ((int32_t)   98)
#define FIX054  ((int32_t)  139)
#define FIX071  ((int32_t)  181)
#define FIX131  ((int32_t)  334)

int dct_fix_matrix[64] =
{
    FIX001,                 FIX001,                 FIX001,           FIX001,           FIX001,                  FIX001,                  FIX001,                 FIX001,
    (FIX001 + FIX131 - FIX038), (FIX071 + FIX131 - FIX038), (FIX071 + FIX038), (FIX038),        -(FIX038),        -(FIX071 + FIX038), -(FIX071 + FIX131 - FIX038), -(FIX001 + FIX131 - FIX038),
    (FIX001 + FIX071), (FIX071),              -(FIX071), -(FIX001 + FIX071), -(FIX001 + FIX071),               -(FIX071), (FIX071), (FIX001 + FIX071),
    (FIX001 + FIX038),     -(FIX071 - FIX038), -(FIX054 + FIX038 + FIX071), -(FIX054 + FIX038), (FIX054 + FIX038), (FIX054 + FIX038 + FIX071), (FIX071 - FIX038),       -(FIX001 + FIX038),
    FIX001,                -FIX001,                -FIX001,           FIX001,           FIX001,                 -FIX001,                 -FIX001,                 FIX001,
    (FIX001 - FIX038),     -(FIX071 + FIX038), (FIX054 + FIX038 - FIX071), (FIX054 + FIX038), -(FIX054 + FIX038), -(FIX054 + FIX038 - FIX071), (FIX071 + FIX038),       -(FIX001 - FIX038),
    (FIX001 - FIX071),              -(FIX071), (FIX071), -(FIX001 - FIX071), -(FIX001 - FIX071),                  FIX071,                 -FIX071, (FIX001 - FIX071),
    (FIX001 - FIX131 + FIX038), -(FIX131 - FIX071 - FIX038), (FIX071 - FIX038),        -(FIX038), (FIX038),        -(FIX071 - FIX038), (FIX131 - FIX038 - FIX071), -(FIX001 - FIX131 + FIX038),
};


#define FLOAT001  ((float)          1.0)
#define FLOAT038  ((float)  0.382683433)
#define FLOAT054  ((float)  0.541196100)
#define FLOAT071  ((float)  0.707106781)
#define FLOAT131  ((float)  1.306562965)

int dct_FLOAT_matrix[64] =
{
    1,                            1,                            1,                   1,                   1,                            1,                            1,                     1,
    (1 + FLOAT131 - FLOAT038), (FLOAT071 + FLOAT131 - FLOAT038), (FLOAT071 + FLOAT038), (FLOAT038),         -(FLOAT038),         -(FLOAT071 + FLOAT038), -(FLOAT071 + FLOAT131 - FLOAT038), -(1 + FLOAT131 - FLOAT038),
    (1 + FLOAT071), (FLOAT071),                  -(FLOAT071),       -(1 + FLOAT071),       -(1 + FLOAT071),                  -(FLOAT071), (FLOAT071), (1 + FLOAT071),
    (1 + FLOAT038),       -(FLOAT071 - FLOAT038), -(FLOAT054 + FLOAT038 + FLOAT071), -(FLOAT054 + FLOAT038), (FLOAT054 + FLOAT038), (FLOAT054 + FLOAT038 + FLOAT071), (FLOAT071 - FLOAT038),         -(1 + FLOAT038),
    1,                           -1,                           -1,                   1,                   1,                           -1,                           -1,                     1,
    (1 - FLOAT038),       -(FLOAT071 + FLOAT038), (FLOAT054 + FLOAT038 - FLOAT071), (FLOAT054 + FLOAT038), -(FLOAT054 + FLOAT038), -(FLOAT054 + FLOAT038 - FLOAT071), (FLOAT071 + FLOAT038),         -(1 - FLOAT038),
    (1 - FLOAT071),                  -(FLOAT071), (FLOAT071),       -(1 - FLOAT071),       -(1 - FLOAT071),                     FLOAT071,                    -FLOAT071, (1 - FLOAT071),
    (1 - FLOAT131 + FLOAT038), -(FLOAT131 - FLOAT071 - FLOAT038), (FLOAT071 - FLOAT038),         -(FLOAT038), (FLOAT038),         -(FLOAT071 - FLOAT038), (FLOAT131 - FLOAT038 - FLOAT071), -(1 - FLOAT131 + FLOAT038),
};

#ifdef ARMC4_OPT

uint32_t zigzag_bank_off[64] =
{
    0x2,     0x6,     0x10002, 0x3f2,   0x10006, 0x0,     0x4,     0x10000,
    0x3f6,   0x103f2, 0x7e2,   0x103f6, 0x3f0,   0x10004, 0x4002,  0x4006,
    0x14002, 0x3f4,   0x103f0, 0x7e6,   0x107e2, 0xbd2,   0x107e6, 0x7e0,
    0x103f4, 0x43f2,  0x14006, 0x4000,  0x4004,  0x14000, 0x43f6,  0x143f2,
    0x7e4,   0x107e0, 0xbd6,   0x10bd2, 0x10bd6, 0xbd0,   0x107e4, 0x47e2,
    0x143f6, 0x43f0,  0x14004, 0x43f4,  0x143f0, 0x47e6,  0x147e2, 0xbd4,
    0x10bd0, 0x10bd4, 0x4bd2,  0x147e6, 0x47e0,  0x143f4, 0x47e4,  0x147e0,
    0x4bd6,  0x14bd2, 0x14bd6, 0x4bd0,  0x147e4, 0x4bd4,  0x14bd0, 0x14bd4,
};

uint16_t zigzag_fdtbl_Y[64];
uint16_t zigzag_fdtbl_UV[64];

#endif

static uint32_t conv_cmd[MAX_CONV_CMD_SIZE];
static uint32_t conv_weight[MAX_WEIGHT_SIZE] __attribute__((aligned(32)));

//static float idct_coef[MAX_IDCT_COEF_SIZE];
//static float dct_coef[MAX_DCT_COEF_SIZE];

typedef struct
{
    uint32_t bitc, bitb;
    uint32_t idx;
    int length;
    uint8_t *buf;
    int realloc;
    int overflow;
} jpeg_buf_t;

#define MAX_TABLE_NUM 3
typedef struct jpeg_encode_context
{
    uint32_t bitc, bitb;
    uint32_t row;
    uint32_t col_num;
    uint32_t start_block;
    uint32_t end_block;
    uint32_t block_idx;
    uint32_t table_idx;
    int table_len;
    uint32_t col;
    uint32_t ch_size;
    uint16_t *mt[MAX_TABLE_NUM];
    int32_t  DC[MAX_TABLE_NUM];
    uint32_t constant;
};

#ifdef USE_SRAM_OPT
    #define JPEG_ENCODE_CONTEXT_ADDR SRAM_OPT_START_ADDR
    #define JPEG_ENCODE_CONTEXT_LEN  sizeof(struct jpeg_encode_context)
    static struct jpeg_encode_context jpeg_asm_encode_context __attribute__((at(JPEG_ENCODE_CONTEXT_ADDR)));
#else
    static struct jpeg_encode_context jpeg_asm_encode_context;
#endif
static struct npu_hw_config jpeg_npu_config;

// Quantization tables
static const uint8_t rb825_table[256] =
{
    0,   0,   0,   0,   0,   1,   1,   1,
    1,   1,   1,   1,   1,   2,   2,   2,
    2,   2,   2,   2,   2,   3,   3,   3,
    3,   3,   3,   3,   3,   4,   4,   4,
    4,   4,   4,   4,   4,   4,   5,   5,
    5,   5,   5,   5,   5,   5,   6,   6,
    6,   6,   6,   6,   6,   6,   7,   7,
    7,   7,   7,   7,   7,   7,   8,   8,
    8,   8,   8,   8,   8,   8,   9,   9,
    9,   9,   9,   9,   9,   9,   9,  10,
    10,  10,  10,  10,  10,  10,  10,  11,
    11,  11,  11,  11,  11,  11,  11,  12,
    12,  12,  12,  12,  12,  12,  12,  13,
    13,  13,  13,  13,  13,  13,  13,  13,
    14,  14,  14,  14,  14,  14,  14,  14,
    15,  15,  15,  15,  15,  15,  15,  15,
    16,  16,  16,  16,  16,  16,  16,  16,
    17,  17,  17,  17,  17,  17,  17,  17,
    18,  18,  18,  18,  18,  18,  18,  18,
    18,  19,  19,  19,  19,  19,  19,  19,
    19,  20,  20,  20,  20,  20,  20,  20,
    20,  21,  21,  21,  21,  21,  21,  21,
    21,  22,  22,  22,  22,  22,  22,  22,
    22,  22,  23,  23,  23,  23,  23,  23,
    23,  23,  24,  24,  24,  24,  24,  24,
    24,  24,  25,  25,  25,  25,  25,  25,
    25,  25,  26,  26,  26,  26,  26,  26,
    26,  26,  27,  27,  27,  27,  27,  27,
    27,  27,  27,  28,  28,  28,  28,  28,
    28,  28,  28,  29,  29,  29,  29,  29,
    29,  29,  29,  30,  30,  30,  30,  30,
    30,  30,  30,  31,  31,  31,  31,  31
};
static const uint8_t g826_table[256] =
{
    0,   0,   0,   1,   1,   1,   1,   2,
    2,   2,   2,   3,   3,   3,   3,   4,
    4,   4,   4,   5,   5,   5,   5,   6,
    6,   6,   6,   7,   7,   7,   7,   8,
    8,   8,   8,   9,   9,   9,   9,  10,
    10,  10,  10,  11,  11,  11,  11,  12,
    12,  12,  12,  13,  13,  13,  13,  14,
    14,  14,  14,  15,  15,  15,  15,  16,
    16,  16,  16,  17,  17,  17,  17,  18,
    18,  18,  18,  19,  19,  19,  19,  20,
    20,  20,  20,  21,  21,  21,  21,  21,
    22,  22,  22,  22,  23,  23,  23,  23,
    24,  24,  24,  24,  25,  25,  25,  25,
    26,  26,  26,  26,  27,  27,  27,  27,
    28,  28,  28,  28,  29,  29,  29,  29,
    30,  30,  30,  30,  31,  31,  31,  31,
    32,  32,  32,  32,  33,  33,  33,  33,
    34,  34,  34,  34,  35,  35,  35,  35,
    36,  36,  36,  36,  37,  37,  37,  37,
    38,  38,  38,  38,  39,  39,  39,  39,
    40,  40,  40,  40,  41,  41,  41,  41,
    42,  42,  42,  42,  42,  43,  43,  43,
    43,  44,  44,  44,  44,  45,  45,  45,
    45,  46,  46,  46,  46,  47,  47,  47,
    47,  48,  48,  48,  48,  49,  49,  49,
    49,  50,  50,  50,  50,  51,  51,  51,
    51,  52,  52,  52,  52,  53,  53,  53,
    53,  54,  54,  54,  54,  55,  55,  55,
    55,  56,  56,  56,  56,  57,  57,  57,
    57,  58,  58,  58,  58,  59,  59,  59,
    59,  60,  60,  60,  60,  61,  61,  61,
    61,  62,  62,  62,  62,  63,  63,  63
};
static int DCY = 0, DCU = 0, DCV = 0;
static float fdtbl_Y[64], fdtbl_UV[64];
static uint8_t YTable[64], UVTable[64];
static float fdtbl_test[4] = {0.0, 0.0, 0.0, 0.0};
/* Convert pos before zigzag into pos after zigzag */
static const uint8_t s_jpeg_ZigZag[64] =
{
    0,  1,   5,  6, 14, 15, 27, 28,
    2,  4,   7, 13, 16, 26, 29, 42,
    3,  8,  12, 17, 25, 30, 41, 43,
    9,  11, 18, 24, 31, 40, 44, 53,
    10, 19, 23, 32, 39, 45, 52, 54,
    20, 22, 33, 38, 46, 51, 55, 60,
    21, 34, 37, 47, 50, 56, 59, 61,
    35, 36, 48, 49, 57, 58, 62, 63
};
/* Convert pos after zigzag into pos before zigzag */
static uint8_t r_jpeg_ZigZag[64];

/*
R0     R1    R2      R3      R4        R5       R6       R7

0,1    4,5,  6,7,    14,15,  26,27     28,29                        R0-R1R2-R3-R4R5
2,3                  12,13                      16,17    42,43      R0R1-R2R3-R6R4-R5R7
       8,9   24,25,          30,31     40,41                        R0R1-R3R6-R2R4-R5R7
10,11                18,19                      44,45    52,53
       22,23 32,33,          38,39     54,55
20,21                46,47                      50,51    60,61
       34,35 36,37           56,57     58,59
48,49                62,63


*/
static const uint8_t YQT[] =
{
    16, 11, 10, 16, 24,  40,  51,  61,
    12, 12, 14, 19, 26,  58,  60,  55,
    14, 13, 16, 24, 40,  57,  69,  56,
    14, 17, 22, 29, 51,  87,  80,  62,
    18, 22, 37, 56, 68,  109, 103, 77,
    24, 35, 55, 64, 81,  104, 113, 92,
    49, 64, 78, 87, 103, 121, 120, 101,
    72, 92, 95, 98, 112, 100, 103, 99
};

static const uint8_t UVQT[] =
{
    17, 18, 24, 47, 99, 99, 99, 99,
    18, 21, 26, 66, 99, 99, 99, 99,
    24, 26, 56, 99, 99, 99, 99, 99,
    47, 66, 99, 99, 99, 99, 99, 99,
    99, 99, 99, 99, 99, 99, 99, 99,
    99, 99, 99, 99, 99, 99, 99, 99,
    99, 99, 99, 99, 99, 99, 99, 99,
    99, 99, 99, 99, 99, 99, 99, 99
};

static const float aasf[] =
{
    1.0f, 1.387039845f, 1.306562965f, 1.175875602f,
    1.0f, 0.785694958f, 0.541196100f, 0.275899379f
};


static const uint8_t std_dc_luminance_nrcodes[] = {0, 0, 1, 5, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0};
static const uint8_t std_dc_luminance_values[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};
static const uint8_t std_ac_luminance_nrcodes[] = {0, 0, 2, 1, 3, 3, 2, 4, 3, 5, 5, 4, 4, 0, 0, 1, 0x7d};
static const uint8_t std_ac_luminance_values[] =
{
    0x01, 0x02, 0x03, 0x00, 0x04, 0x11, 0x05, 0x12, 0x21, 0x31, 0x41, 0x06, 0x13, 0x51, 0x61, 0x07, 0x22, 0x71, 0x14, 0x32, 0x81, 0x91, 0xa1, 0x08,
    0x23, 0x42, 0xb1, 0xc1, 0x15, 0x52, 0xd1, 0xf0, 0x24, 0x33, 0x62, 0x72, 0x82, 0x09, 0x0a, 0x16, 0x17, 0x18, 0x19, 0x1a, 0x25, 0x26, 0x27, 0x28,
    0x29, 0x2a, 0x34, 0x35, 0x36, 0x37, 0x38, 0x39, 0x3a, 0x43, 0x44, 0x45, 0x46, 0x47, 0x48, 0x49, 0x4a, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58, 0x59,
    0x5a, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6a, 0x73, 0x74, 0x75, 0x76, 0x77, 0x78, 0x79, 0x7a, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89,
    0x8a, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99, 0x9a, 0xa2, 0xa3, 0xa4, 0xa5, 0xa6, 0xa7, 0xa8, 0xa9, 0xaa, 0xb2, 0xb3, 0xb4, 0xb5, 0xb6,
    0xb7, 0xb8, 0xb9, 0xba, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda, 0xe1, 0xe2,
    0xe3, 0xe4, 0xe5, 0xe6, 0xe7, 0xe8, 0xe9, 0xea, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8, 0xf9, 0xfa
};

static const uint8_t std_dc_chrominance_nrcodes[] = {0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0};
static const uint8_t std_dc_chrominance_values[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};
static const uint8_t std_ac_chrominance_nrcodes[] = {0, 0, 2, 1, 2, 4, 4, 3, 4, 7, 5, 4, 4, 0, 1, 2, 0x77};
static const uint8_t std_ac_chrominance_values[] =
{
    0x00, 0x01, 0x02, 0x03, 0x11, 0x04, 0x05, 0x21, 0x31, 0x06, 0x12, 0x41, 0x51, 0x07, 0x61, 0x71, 0x13, 0x22, 0x32, 0x81, 0x08, 0x14, 0x42, 0x91,
    0xa1, 0xb1, 0xc1, 0x09, 0x23, 0x33, 0x52, 0xf0, 0x15, 0x62, 0x72, 0xd1, 0x0a, 0x16, 0x24, 0x34, 0xe1, 0x25, 0xf1, 0x17, 0x18, 0x19, 0x1a, 0x26,
    0x27, 0x28, 0x29, 0x2a, 0x35, 0x36, 0x37, 0x38, 0x39, 0x3a, 0x43, 0x44, 0x45, 0x46, 0x47, 0x48, 0x49, 0x4a, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58,
    0x59, 0x5a, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6a, 0x73, 0x74, 0x75, 0x76, 0x77, 0x78, 0x79, 0x7a, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87,
    0x88, 0x89, 0x8a, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99, 0x9a, 0xa2, 0xa3, 0xa4, 0xa5, 0xa6, 0xa7, 0xa8, 0xa9, 0xaa, 0xb2, 0xb3, 0xb4,
    0xb5, 0xb6, 0xb7, 0xb8, 0xb9, 0xba, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda,
    0xe2, 0xe3, 0xe4, 0xe5, 0xe6, 0xe7, 0xe8, 0xe9, 0xea, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8, 0xf9, 0xfa
};

// Huffman tables
static const uint16_t YDC_HT[12][2] = { {0, 2}, {2, 3}, {3, 3}, {4, 3}, {5, 3}, {6, 3}, {14, 4}, {30, 5}, {62, 6}, {126, 7}, {254, 8}, {510, 9}};
static const uint16_t UVDC_HT[12][2] = { {0, 2}, {1, 2}, {2, 2}, {6, 3}, {14, 4}, {30, 5}, {62, 6}, {126, 7}, {254, 8}, {510, 9}, {1022, 10}, {2046, 11}};
static const uint16_t YAC_HT[256][2] =
{
    {0x000A, 0x0004}, {0x0000, 0x0002}, {0x0001, 0x0002}, {0x0004, 0x0003}, {0x000B, 0x0004}, {0x001A, 0x0005}, {0x0078, 0x0007}, {0x00F8, 0x0008},
    {0x03F6, 0x000A}, {0xFF82, 0x0010}, {0xFF83, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x000C, 0x0004}, {0x001B, 0x0005}, {0x0079, 0x0007}, {0x01F6, 0x0009}, {0x07F6, 0x000B}, {0xFF84, 0x0010}, {0xFF85, 0x0010},
    {0xFF86, 0x0010}, {0xFF87, 0x0010}, {0xFF88, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001C, 0x0005}, {0x00F9, 0x0008}, {0x03F7, 0x000A}, {0x0FF4, 0x000C}, {0xFF89, 0x0010}, {0xFF8A, 0x0010}, {0xFF8B, 0x0010},
    {0xFF8C, 0x0010}, {0xFF8D, 0x0010}, {0xFF8E, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003A, 0x0006}, {0x01F7, 0x0009}, {0x0FF5, 0x000C}, {0xFF8F, 0x0010}, {0xFF90, 0x0010}, {0xFF91, 0x0010}, {0xFF92, 0x0010},
    {0xFF93, 0x0010}, {0xFF94, 0x0010}, {0xFF95, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003B, 0x0006}, {0x03F8, 0x000A}, {0xFF96, 0x0010}, {0xFF97, 0x0010}, {0xFF98, 0x0010}, {0xFF99, 0x0010}, {0xFF9A, 0x0010},
    {0xFF9B, 0x0010}, {0xFF9C, 0x0010}, {0xFF9D, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007A, 0x0007}, {0x07F7, 0x000B}, {0xFF9E, 0x0010}, {0xFF9F, 0x0010}, {0xFFA0, 0x0010}, {0xFFA1, 0x0010}, {0xFFA2, 0x0010},
    {0xFFA3, 0x0010}, {0xFFA4, 0x0010}, {0xFFA5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007B, 0x0007}, {0x0FF6, 0x000C}, {0xFFA6, 0x0010}, {0xFFA7, 0x0010}, {0xFFA8, 0x0010}, {0xFFA9, 0x0010}, {0xFFAA, 0x0010},
    {0xFFAB, 0x0010}, {0xFFAC, 0x0010}, {0xFFAD, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x00FA, 0x0008}, {0x0FF7, 0x000C}, {0xFFAE, 0x0010}, {0xFFAF, 0x0010}, {0xFFB0, 0x0010}, {0xFFB1, 0x0010}, {0xFFB2, 0x0010},
    {0xFFB3, 0x0010}, {0xFFB4, 0x0010}, {0xFFB5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F8, 0x0009}, {0x7FC0, 0x000F}, {0xFFB6, 0x0010}, {0xFFB7, 0x0010}, {0xFFB8, 0x0010}, {0xFFB9, 0x0010}, {0xFFBA, 0x0010},
    {0xFFBB, 0x0010}, {0xFFBC, 0x0010}, {0xFFBD, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F9, 0x0009}, {0xFFBE, 0x0010}, {0xFFBF, 0x0010}, {0xFFC0, 0x0010}, {0xFFC1, 0x0010}, {0xFFC2, 0x0010}, {0xFFC3, 0x0010},
    {0xFFC4, 0x0010}, {0xFFC5, 0x0010}, {0xFFC6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01FA, 0x0009}, {0xFFC7, 0x0010}, {0xFFC8, 0x0010}, {0xFFC9, 0x0010}, {0xFFCA, 0x0010}, {0xFFCB, 0x0010}, {0xFFCC, 0x0010},
    {0xFFCD, 0x0010}, {0xFFCE, 0x0010}, {0xFFCF, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x03F9, 0x000A}, {0xFFD0, 0x0010}, {0xFFD1, 0x0010}, {0xFFD2, 0x0010}, {0xFFD3, 0x0010}, {0xFFD4, 0x0010}, {0xFFD5, 0x0010},
    {0xFFD6, 0x0010}, {0xFFD7, 0x0010}, {0xFFD8, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x03FA, 0x000A}, {0xFFD9, 0x0010}, {0xFFDA, 0x0010}, {0xFFDB, 0x0010}, {0xFFDC, 0x0010}, {0xFFDD, 0x0010}, {0xFFDE, 0x0010},
    {0xFFDF, 0x0010}, {0xFFE0, 0x0010}, {0xFFE1, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x07F8, 0x000B}, {0xFFE2, 0x0010}, {0xFFE3, 0x0010}, {0xFFE4, 0x0010}, {0xFFE5, 0x0010}, {0xFFE6, 0x0010}, {0xFFE7, 0x0010},
    {0xFFE8, 0x0010}, {0xFFE9, 0x0010}, {0xFFEA, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0xFFEB, 0x0010}, {0xFFEC, 0x0010}, {0xFFED, 0x0010}, {0xFFEE, 0x0010}, {0xFFEF, 0x0010}, {0xFFF0, 0x0010}, {0xFFF1, 0x0010},
    {0xFFF2, 0x0010}, {0xFFF3, 0x0010}, {0xFFF4, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x07F9, 0x000B}, {0xFFF5, 0x0010}, {0xFFF6, 0x0010}, {0xFFF7, 0x0010}, {0xFFF8, 0x0010}, {0xFFF9, 0x0010}, {0xFFFA, 0x0010}, {0xFFFB, 0x0010},
    {0xFFFC, 0x0010}, {0xFFFD, 0x0010}, {0xFFFE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
};
/* Mix y huffman ac/dc table and y qt table together */
#ifdef USE_SRAM_OPT
#define Y_MT_ADDR (JPEG_ENCODE_CONTEXT_ADDR+JPEG_ENCODE_CONTEXT_LEN)
#define Y_MT_LEN  ((256+16+128)*2*2)
static uint16_t Y_MT[256 + 16 + 128][2] __attribute__((at(Y_MT_ADDR))) =
{
#else
static uint16_t Y_MT[256 + 16 + 128][2] =
{
#endif
    {0x000A, 0x0004}, {0x0000, 0x0002}, {0x0001, 0x0002}, {0x0004, 0x0003}, {0x000B, 0x0004}, {0x001A, 0x0005}, {0x0078, 0x0007}, {0x00F8, 0x0008},
    {0x03F6, 0x000A}, {0xFF82, 0x0010}, {0xFF83, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x000C, 0x0004}, {0x001B, 0x0005}, {0x0079, 0x0007}, {0x01F6, 0x0009}, {0x07F6, 0x000B}, {0xFF84, 0x0010}, {0xFF85, 0x0010},
    {0xFF86, 0x0010}, {0xFF87, 0x0010}, {0xFF88, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001C, 0x0005}, {0x00F9, 0x0008}, {0x03F7, 0x000A}, {0x0FF4, 0x000C}, {0xFF89, 0x0010}, {0xFF8A, 0x0010}, {0xFF8B, 0x0010},
    {0xFF8C, 0x0010}, {0xFF8D, 0x0010}, {0xFF8E, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003A, 0x0006}, {0x01F7, 0x0009}, {0x0FF5, 0x000C}, {0xFF8F, 0x0010}, {0xFF90, 0x0010}, {0xFF91, 0x0010}, {0xFF92, 0x0010},
    {0xFF93, 0x0010}, {0xFF94, 0x0010}, {0xFF95, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003B, 0x0006}, {0x03F8, 0x000A}, {0xFF96, 0x0010}, {0xFF97, 0x0010}, {0xFF98, 0x0010}, {0xFF99, 0x0010}, {0xFF9A, 0x0010},
    {0xFF9B, 0x0010}, {0xFF9C, 0x0010}, {0xFF9D, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007A, 0x0007}, {0x07F7, 0x000B}, {0xFF9E, 0x0010}, {0xFF9F, 0x0010}, {0xFFA0, 0x0010}, {0xFFA1, 0x0010}, {0xFFA2, 0x0010},
    {0xFFA3, 0x0010}, {0xFFA4, 0x0010}, {0xFFA5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007B, 0x0007}, {0x0FF6, 0x000C}, {0xFFA6, 0x0010}, {0xFFA7, 0x0010}, {0xFFA8, 0x0010}, {0xFFA9, 0x0010}, {0xFFAA, 0x0010},
    {0xFFAB, 0x0010}, {0xFFAC, 0x0010}, {0xFFAD, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x00FA, 0x0008}, {0x0FF7, 0x000C}, {0xFFAE, 0x0010}, {0xFFAF, 0x0010}, {0xFFB0, 0x0010}, {0xFFB1, 0x0010}, {0xFFB2, 0x0010},
    {0xFFB3, 0x0010}, {0xFFB4, 0x0010}, {0xFFB5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F8, 0x0009}, {0x7FC0, 0x000F}, {0xFFB6, 0x0010}, {0xFFB7, 0x0010}, {0xFFB8, 0x0010}, {0xFFB9, 0x0010}, {0xFFBA, 0x0010},
    {0xFFBB, 0x0010}, {0xFFBC, 0x0010}, {0xFFBD, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F9, 0x0009}, {0xFFBE, 0x0010}, {0xFFBF, 0x0010}, {0xFFC0, 0x0010}, {0xFFC1, 0x0010}, {0xFFC2, 0x0010}, {0xFFC3, 0x0010},
    {0xFFC4, 0x0010}, {0xFFC5, 0x0010}, {0xFFC6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01FA, 0x0009}, {0xFFC7, 0x0010}, {0xFFC8, 0x0010}, {0xFFC9, 0x0010}, {0xFFCA, 0x0010}, {0xFFCB, 0x0010}, {0xFFCC, 0x0010},
    {0xFFCD, 0x0010}, {0xFFCE, 0x0010}, {0xFFCF, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x03F9, 0x000A}, {0xFFD0, 0x0010}, {0xFFD1, 0x0010}, {0xFFD2, 0x0010}, {0xFFD3, 0x0010}, {0xFFD4, 0x0010}, {0xFFD5, 0x0010},
    {0xFFD6, 0x0010}, {0xFFD7, 0x0010}, {0xFFD8, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x03FA, 0x000A}, {0xFFD9, 0x0010}, {0xFFDA, 0x0010}, {0xFFDB, 0x0010}, {0xFFDC, 0x0010}, {0xFFDD, 0x0010}, {0xFFDE, 0x0010},
    {0xFFDF, 0x0010}, {0xFFE0, 0x0010}, {0xFFE1, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x07F8, 0x000B}, {0xFFE2, 0x0010}, {0xFFE3, 0x0010}, {0xFFE4, 0x0010}, {0xFFE5, 0x0010}, {0xFFE6, 0x0010}, {0xFFE7, 0x0010},
    {0xFFE8, 0x0010}, {0xFFE9, 0x0010}, {0xFFEA, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0xFFEB, 0x0010}, {0xFFEC, 0x0010}, {0xFFED, 0x0010}, {0xFFEE, 0x0010}, {0xFFEF, 0x0010}, {0xFFF0, 0x0010}, {0xFFF1, 0x0010},
    {0xFFF2, 0x0010}, {0xFFF3, 0x0010}, {0xFFF4, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x07F9, 0x000B}, {0xFFF5, 0x0010}, {0xFFF6, 0x0010}, {0xFFF7, 0x0010}, {0xFFF8, 0x0010}, {0xFFF9, 0x0010}, {0xFFFA, 0x0010}, {0xFFFB, 0x0010},
    {0xFFFC, 0x0010}, {0xFFFD, 0x0010}, {0xFFFE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0, 2},           {2, 3},           {3, 3},           {4, 3},           {5, 3},           {6, 3},           {14, 4},          {30, 5},
    {62, 6},          {126, 7},         {254, 8},         {510, 9},         {0, 0},           {0, 0},           {0, 0},           {0, 0},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
};

static uint16_t UVAC_HT[256][2] =
{
    {0x0000, 0x0002}, {0x0001, 0x0002}, {0x0004, 0x0003}, {0x000A, 0x0004}, {0x0018, 0x0005}, {0x0019, 0x0005}, {0x0038, 0x0006}, {0x0078, 0x0007},
    {0x01F4, 0x0009}, {0x03F6, 0x000A}, {0x0FF4, 0x000C}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x000B, 0x0004}, {0x0039, 0x0006}, {0x00F6, 0x0008}, {0x01F5, 0x0009}, {0x07F6, 0x000B}, {0x0FF5, 0x000C}, {0xFF88, 0x0010},
    {0xFF89, 0x0010}, {0xFF8A, 0x0010}, {0xFF8B, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001A, 0x0005}, {0x00F7, 0x0008}, {0x03F7, 0x000A}, {0x0FF6, 0x000C}, {0x7FC2, 0x000F}, {0xFF8C, 0x0010}, {0xFF8D, 0x0010},
    {0xFF8E, 0x0010}, {0xFF8F, 0x0010}, {0xFF90, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001B, 0x0005}, {0x00F8, 0x0008}, {0x03F8, 0x000A}, {0x0FF7, 0x000C}, {0xFF91, 0x0010}, {0xFF92, 0x0010}, {0xFF93, 0x0010},
    {0xFF94, 0x0010}, {0xFF95, 0x0010}, {0xFF96, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003A, 0x0006}, {0x01F6, 0x0009}, {0xFF97, 0x0010}, {0xFF98, 0x0010}, {0xFF99, 0x0010}, {0xFF9A, 0x0010}, {0xFF9B, 0x0010},
    {0xFF9C, 0x0010}, {0xFF9D, 0x0010}, {0xFF9E, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003B, 0x0006}, {0x03F9, 0x000A}, {0xFF9F, 0x0010}, {0xFFA0, 0x0010}, {0xFFA1, 0x0010}, {0xFFA2, 0x0010}, {0xFFA3, 0x0010},
    {0xFFA4, 0x0010}, {0xFFA5, 0x0010}, {0xFFA6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x0079, 0x0007}, {0x07F7, 0x000B}, {0xFFA7, 0x0010}, {0xFFA8, 0x0010}, {0xFFA9, 0x0010}, {0xFFAA, 0x0010}, {0xFFAB, 0x0010},
    {0xFFAC, 0x0010}, {0xFFAD, 0x0010}, {0xFFAE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007A, 0x0007}, {0x07F8, 0x000B}, {0xFFAF, 0x0010}, {0xFFB0, 0x0010}, {0xFFB1, 0x0010}, {0xFFB2, 0x0010}, {0xFFB3, 0x0010},
    {0xFFB4, 0x0010}, {0xFFB5, 0x0010}, {0xFFB6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x00F9, 0x0008}, {0xFFB7, 0x0010}, {0xFFB8, 0x0010}, {0xFFB9, 0x0010}, {0xFFBA, 0x0010}, {0xFFBB, 0x0010}, {0xFFBC, 0x0010},
    {0xFFBD, 0x0010}, {0xFFBE, 0x0010}, {0xFFBF, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F7, 0x0009}, {0xFFC0, 0x0010}, {0xFFC1, 0x0010}, {0xFFC2, 0x0010}, {0xFFC3, 0x0010}, {0xFFC4, 0x0010}, {0xFFC5, 0x0010},
    {0xFFC6, 0x0010}, {0xFFC7, 0x0010}, {0xFFC8, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F8, 0x0009}, {0xFFC9, 0x0010}, {0xFFCA, 0x0010}, {0xFFCB, 0x0010}, {0xFFCC, 0x0010}, {0xFFCD, 0x0010}, {0xFFCE, 0x0010},
    {0xFFCF, 0x0010}, {0xFFD0, 0x0010}, {0xFFD1, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F9, 0x0009}, {0xFFD2, 0x0010}, {0xFFD3, 0x0010}, {0xFFD4, 0x0010}, {0xFFD5, 0x0010}, {0xFFD6, 0x0010}, {0xFFD7, 0x0010},
    {0xFFD8, 0x0010}, {0xFFD9, 0x0010}, {0xFFDA, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01FA, 0x0009}, {0xFFDB, 0x0010}, {0xFFDC, 0x0010}, {0xFFDD, 0x0010}, {0xFFDE, 0x0010}, {0xFFDF, 0x0010}, {0xFFE0, 0x0010},
    {0xFFE1, 0x0010}, {0xFFE2, 0x0010}, {0xFFE3, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x07F9, 0x000B}, {0xFFE4, 0x0010}, {0xFFE5, 0x0010}, {0xFFE6, 0x0010}, {0xFFE7, 0x0010}, {0xFFE8, 0x0010}, {0xFFE9, 0x0010},
    {0xFFEA, 0x0010}, {0xFFEB, 0x0010}, {0xFFEC, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x3FE0, 0x000E}, {0xFFED, 0x0010}, {0xFFEE, 0x0010}, {0xFFEF, 0x0010}, {0xFFF0, 0x0010}, {0xFFF1, 0x0010}, {0xFFF2, 0x0010},
    {0xFFF3, 0x0010}, {0xFFF4, 0x0010}, {0xFFF5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x03FA, 0x000A}, {0x7FC3, 0x000F}, {0xFFF6, 0x0010}, {0xFFF7, 0x0010}, {0xFFF8, 0x0010}, {0xFFF9, 0x0010}, {0xFFFA, 0x0010}, {0xFFFB, 0x0010},
    {0xFFFC, 0x0010}, {0xFFFD, 0x0010}, {0xFFFE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
};

#ifdef USE_SRAM_OPT
#define UV_MT_ADDR (Y_MT_ADDR+Y_MT_LEN)
#define UV_MT_LEN  ((256+16+128)*2*2)
static uint16_t UV_MT[256 + 16 + 128][2] __attribute__((at(UV_MT_ADDR))) =
{
#else
static uint16_t UV_MT[256 + 16 + 128][2] =
{
#endif
    {0x0000, 0x0002}, {0x0001, 0x0002}, {0x0004, 0x0003}, {0x000A, 0x0004}, {0x0018, 0x0005}, {0x0019, 0x0005}, {0x0038, 0x0006}, {0x0078, 0x0007},
    {0x01F4, 0x0009}, {0x03F6, 0x000A}, {0x0FF4, 0x000C}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x000B, 0x0004}, {0x0039, 0x0006}, {0x00F6, 0x0008}, {0x01F5, 0x0009}, {0x07F6, 0x000B}, {0x0FF5, 0x000C}, {0xFF88, 0x0010},
    {0xFF89, 0x0010}, {0xFF8A, 0x0010}, {0xFF8B, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001A, 0x0005}, {0x00F7, 0x0008}, {0x03F7, 0x000A}, {0x0FF6, 0x000C}, {0x7FC2, 0x000F}, {0xFF8C, 0x0010}, {0xFF8D, 0x0010},
    {0xFF8E, 0x0010}, {0xFF8F, 0x0010}, {0xFF90, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x001B, 0x0005}, {0x00F8, 0x0008}, {0x03F8, 0x000A}, {0x0FF7, 0x000C}, {0xFF91, 0x0010}, {0xFF92, 0x0010}, {0xFF93, 0x0010},
    {0xFF94, 0x0010}, {0xFF95, 0x0010}, {0xFF96, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003A, 0x0006}, {0x01F6, 0x0009}, {0xFF97, 0x0010}, {0xFF98, 0x0010}, {0xFF99, 0x0010}, {0xFF9A, 0x0010}, {0xFF9B, 0x0010},
    {0xFF9C, 0x0010}, {0xFF9D, 0x0010}, {0xFF9E, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x003B, 0x0006}, {0x03F9, 0x000A}, {0xFF9F, 0x0010}, {0xFFA0, 0x0010}, {0xFFA1, 0x0010}, {0xFFA2, 0x0010}, {0xFFA3, 0x0010},
    {0xFFA4, 0x0010}, {0xFFA5, 0x0010}, {0xFFA6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x0079, 0x0007}, {0x07F7, 0x000B}, {0xFFA7, 0x0010}, {0xFFA8, 0x0010}, {0xFFA9, 0x0010}, {0xFFAA, 0x0010}, {0xFFAB, 0x0010},
    {0xFFAC, 0x0010}, {0xFFAD, 0x0010}, {0xFFAE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x007A, 0x0007}, {0x07F8, 0x000B}, {0xFFAF, 0x0010}, {0xFFB0, 0x0010}, {0xFFB1, 0x0010}, {0xFFB2, 0x0010}, {0xFFB3, 0x0010},
    {0xFFB4, 0x0010}, {0xFFB5, 0x0010}, {0xFFB6, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x00F9, 0x0008}, {0xFFB7, 0x0010}, {0xFFB8, 0x0010}, {0xFFB9, 0x0010}, {0xFFBA, 0x0010}, {0xFFBB, 0x0010}, {0xFFBC, 0x0010},
    {0xFFBD, 0x0010}, {0xFFBE, 0x0010}, {0xFFBF, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F7, 0x0009}, {0xFFC0, 0x0010}, {0xFFC1, 0x0010}, {0xFFC2, 0x0010}, {0xFFC3, 0x0010}, {0xFFC4, 0x0010}, {0xFFC5, 0x0010},
    {0xFFC6, 0x0010}, {0xFFC7, 0x0010}, {0xFFC8, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F8, 0x0009}, {0xFFC9, 0x0010}, {0xFFCA, 0x0010}, {0xFFCB, 0x0010}, {0xFFCC, 0x0010}, {0xFFCD, 0x0010}, {0xFFCE, 0x0010},
    {0xFFCF, 0x0010}, {0xFFD0, 0x0010}, {0xFFD1, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01F9, 0x0009}, {0xFFD2, 0x0010}, {0xFFD3, 0x0010}, {0xFFD4, 0x0010}, {0xFFD5, 0x0010}, {0xFFD6, 0x0010}, {0xFFD7, 0x0010},
    {0xFFD8, 0x0010}, {0xFFD9, 0x0010}, {0xFFDA, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x01FA, 0x0009}, {0xFFDB, 0x0010}, {0xFFDC, 0x0010}, {0xFFDD, 0x0010}, {0xFFDE, 0x0010}, {0xFFDF, 0x0010}, {0xFFE0, 0x0010},
    {0xFFE1, 0x0010}, {0xFFE2, 0x0010}, {0xFFE3, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x07F9, 0x000B}, {0xFFE4, 0x0010}, {0xFFE5, 0x0010}, {0xFFE6, 0x0010}, {0xFFE7, 0x0010}, {0xFFE8, 0x0010}, {0xFFE9, 0x0010},
    {0xFFEA, 0x0010}, {0xFFEB, 0x0010}, {0xFFEC, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x0000, 0x0000}, {0x3FE0, 0x000E}, {0xFFED, 0x0010}, {0xFFEE, 0x0010}, {0xFFEF, 0x0010}, {0xFFF0, 0x0010}, {0xFFF1, 0x0010}, {0xFFF2, 0x0010},
    {0xFFF3, 0x0010}, {0xFFF4, 0x0010}, {0xFFF5, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0x03FA, 0x000A}, {0x7FC3, 0x000F}, {0xFFF6, 0x0010}, {0xFFF7, 0x0010}, {0xFFF8, 0x0010}, {0xFFF9, 0x0010}, {0xFFFA, 0x0010}, {0xFFFB, 0x0010},
    {0xFFFC, 0x0010}, {0xFFFD, 0x0010}, {0xFFFE, 0x0010}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000}, {0x0000, 0x0000},
    {0, 2},           {1, 2},           {2, 2},           {6, 3},           {14, 4},          {30, 5},          {62, 6},          {126, 7},
    {254, 8},         {510, 9},         {1022, 10},       {2046, 11},       {0, 0},           {0, 0},           {0, 0},           {0, 0},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
    {16, 11},        {10, 16},        {24,  40},       {51,  61},       {12, 12},        {14, 19},        {26,  58},       {60,  55},
};
#define ERROR_GET_BLOCK 1

static volatile int jpeg_errno;
static void report_error(int errno)
{
    //printf("error = %d\n", errno);
    jpeg_errno = errno;
    return;
}

static void jpeg_put_char(jpeg_buf_t *jpeg_buf, char c)
{
    jpeg_buf->buf[jpeg_buf->idx++] = c;
}

static void jpeg_put_bytes(jpeg_buf_t *jpeg_buf, const uint8_t *data, int size)
{
    memcpy(jpeg_buf->buf + jpeg_buf->idx, data, size);
    jpeg_buf->idx += size;
}

static void jpeg_writeBits(jpeg_buf_t *jpeg_buf, const uint16_t *bs)
{
    jpeg_buf->bitc += bs[1];
    jpeg_buf->bitb |= bs[0] << (24 - jpeg_buf->bitc);

    while (jpeg_buf->bitc > 7)
    {
        uint8_t c = (jpeg_buf->bitb >> 16) & 255;
        jpeg_put_char(jpeg_buf, c);
        if (c == 255)
        {
            jpeg_put_char(jpeg_buf, 0);
        }
        jpeg_buf->bitb <<= 8;
        jpeg_buf->bitc -= 8;
    }
}

static int roundf_c(float x)
{
    //int n;
    float r = x - (int) x;
    return (x > 0) ? ((r > 0.49999f) ? (int)(x + 0.5f) : (int)x) : ((r < -0.49999f) ? ((int)(x - 0.5f)) : (int)x);
}

#ifdef ARMC4_OPT

//Huffman-encoded magnitude value
static void jpeg_calcBits(int val, uint16_t bits[2])
{
    int t1 = val;
    if (val < 0)
    {
        t1 = -val;
        val = val - 1;
    }
    bits[1] = 32 - __CLZ(t1);
    bits[0] = val & ((1 << bits[1]) - 1); /* 0xD0 0xFF30 valid bits is 8, but 0x30 valid bits is 6 */
}

int __asm inline fast_roundf_in(float x)
{
    VCVTR.S32.F32 S2, S0
    VMOV  R0, S2
    BX LR
}

static int  inline fast_roundf(float x)
{
    return fast_roundf_in(x);
}

#else

static void jpeg_calcBits(int val, unsigned short bits[2])
{
    int tmp1 = val < 0 ? -val : val;
    val = val < 0 ? val - 1 : val;
    bits[1] = 1;
    while (tmp1 >>= 1)
    {
        ++bits[1];
    }
    bits[0] = val & ((1 << bits[1]) - 1);
}

static int fast_roundf(float x)
{
    return roundf_c(x);
}
#endif

static void jpeg_init(int quality)
{
    static int q = 0;

    quality = quality < 50 ? 5000 / quality : 200 - quality * 2;

    // If quality changed, update quantization matrix
    if (q != quality)
    {
        q = quality;
        for (int i = 0; i < 64; ++i)
        {
            int yti = (YQT[i] * quality + 50) / 100;
            YTable[s_jpeg_ZigZag[i]] = yti < 1 ? 1 : yti > 255 ? 255 : yti;
            int uvti  = (UVQT[i] * quality + 50) / 100;
            UVTable[s_jpeg_ZigZag[i]] = uvti < 1 ? 1 : uvti > 255 ? 255 : uvti;
        }

        for (int r = 0, k = 0; r < 8; ++r)
        {
            for (int c = 0; c < 8; ++c, ++k)
            {
                fdtbl_Y[k]  = 1.0f / (aasf[r] * aasf[c] * YTable [s_jpeg_ZigZag[k]] * 8.0f);
                fdtbl_UV[k] = 1.0f / (aasf[r] * aasf[c] * UVTable[s_jpeg_ZigZag[k]] * 8.0f);
                //((uint32_t *)Y_MT)[256 + 16 + s_jpeg_ZigZag[k]]  = ((uint32_t)(fdtbl_Y[k] * ((float)0x40000)));
                //((uint32_t *)UV_MT)[256 + 16 + s_jpeg_ZigZag[k]] = ((uint32_t)(fdtbl_UV[k]* ((float)0x40000)));
#ifdef ASM_PUT_BLOCK_SHORT_LOOP
                ((int32_t *)Y_MT)[256 + 16 + 2 * s_jpeg_ZigZag[k] + 0] = zigzag_bank_off[s_jpeg_ZigZag[k]];
                ((int32_t *)UV_MT)[256 + 16 + 2 * s_jpeg_ZigZag[k] + 0] = zigzag_bank_off[s_jpeg_ZigZag[k]];
                ((int32_t *)Y_MT)[256 + 16 + 2 * s_jpeg_ZigZag[k] + 1]  = ((int32_t)roundf_c(fdtbl_Y[k] * ((float)0x100000000)));
                ((int32_t *)UV_MT)[256 + 16 + 2 * s_jpeg_ZigZag[k] + 1] = ((int32_t)roundf_c(fdtbl_UV[k] * ((float)0x100000000)));
#else
                ((int32_t *)Y_MT)[256 + 16 + s_jpeg_ZigZag[k]]  = ((int32_t)roundf_c(fdtbl_Y[k] * ((float)0x10000000)));
                ((int32_t *)UV_MT)[256 + 16 + s_jpeg_ZigZag[k]] = ((int32_t)roundf_c(fdtbl_UV[k] * ((float)0x10000000)));
#endif
            }
        }
    }
}

static void jpeg_write_headers(jpeg_buf_t *jpeg_buf, int w, int h, int bpp, jpeg_subsample_t jpeg_subsample)
{
    // Number of components (1 or 3)
    uint8_t nr_comp = (bpp == 1) ? 1 : 3;

    // JPEG headers
    uint8_t m_soi[] =
    {
        0xFF, 0xD8          // SOI
    };

    uint8_t m_app0[] =
    {
        0xFF, 0xE0,         // APP0
        0x00, 0x10,  'J',  'F',  'I',  'F', 0x00, 0x01,
        0x01, 0x00, 0x00, 0x01, 0x00, 0x01, 0x00, 0x00
    };

    uint8_t m_dqt[] =
    {
        0xFF, 0xDB,         // DQT
        (bpp * 65 + 2) >> 8, // Header length MSB
                       (bpp * 65 + 2) & 0xFF, // Header length LSB
    };

    uint8_t m_sof0[] =
    {
        0xFF, 0xC0,         // SOF0
        (nr_comp * 3 + 8) >> 8, // Header length MSB
                          (nr_comp * 3 + 8) & 0xFF, // Header length LSB
                          0x08,               // Bits per sample
                          h >> 8, h & 0xFF,   // Height
                          w >> 8, w & 0xFF,   // Width
                          nr_comp,            // Number of components
    };

    uint8_t m_dht[] =
    {
        0xFF, 0xC4,         // DHT
        (bpp * 208 + 2) >> 8, // Header length MSB
                        (bpp * 208 + 2) & 0xFF, // Header length LSB
    };

    uint8_t m_sos[] =
    {
        0xFF, 0xDA,         // SOS
        (nr_comp * 2 + 6) >> 8, // Header length MSB
                          (nr_comp * 2 + 6) & 0xFF, // Header length LSB
                          nr_comp,            // Number of components
    };

    // Write SOI marker
    jpeg_put_bytes(jpeg_buf, m_soi, sizeof(m_soi));
    // Write APP0 marker
    jpeg_put_bytes(jpeg_buf, m_app0, sizeof(m_app0));

    // Write DQT marker
    jpeg_put_bytes(jpeg_buf, m_dqt, sizeof(m_dqt));
    // Write Y quantization table (index, table)
    jpeg_put_char(jpeg_buf, 0);
    jpeg_put_bytes(jpeg_buf, YTable, sizeof(YTable));

    if (bpp > 1)
    {
        // Write UV quantization table (index, table)
        jpeg_put_char(jpeg_buf, 1);
        jpeg_put_bytes(jpeg_buf, UVTable, sizeof(UVTable));
    }

    // Write SOF0 marker
    jpeg_put_bytes(jpeg_buf, m_sof0, sizeof(m_sof0));
    for (int i = 0; i < nr_comp; i++)
    {
        // Component ID, HV sampling, q table idx
        jpeg_put_bytes(jpeg_buf, (uint8_t [3])
        {
            i + 1, (i == 0 && bpp == 2) ? jpeg_subsample : 0x11, (i > 0)
        }, 3);

    }

    // Write DHT marker
    jpeg_put_bytes(jpeg_buf, m_dht, sizeof(m_dht));

    // Write DHT-YDC
    jpeg_put_char(jpeg_buf, 0x00);
    jpeg_put_bytes(jpeg_buf, std_dc_luminance_nrcodes + 1, sizeof(std_dc_luminance_nrcodes) - 1);
    jpeg_put_bytes(jpeg_buf, std_dc_luminance_values, sizeof(std_dc_luminance_values));

    // Write DHT-YAC
    jpeg_put_char(jpeg_buf, 0x10);
    jpeg_put_bytes(jpeg_buf, std_ac_luminance_nrcodes + 1, sizeof(std_ac_luminance_nrcodes) - 1);
    jpeg_put_bytes(jpeg_buf, std_ac_luminance_values, sizeof(std_ac_luminance_values));

    if (bpp > 1)
    {
        // Write DHT-UDC
        jpeg_put_char(jpeg_buf, 0x01);
        jpeg_put_bytes(jpeg_buf, std_dc_chrominance_nrcodes + 1, sizeof(std_dc_chrominance_nrcodes) - 1);
        jpeg_put_bytes(jpeg_buf, std_dc_chrominance_values, sizeof(std_dc_chrominance_values));

        // Write DHT-UAC
        jpeg_put_char(jpeg_buf, 0x11);
        jpeg_put_bytes(jpeg_buf, std_ac_chrominance_nrcodes + 1, sizeof(std_ac_chrominance_nrcodes) - 1);
        jpeg_put_bytes(jpeg_buf, std_ac_chrominance_values, sizeof(std_ac_chrominance_values));
    }

    // Write SOS marker
    jpeg_put_bytes(jpeg_buf, m_sos, sizeof(m_sos));
    for (int i = 0; i < nr_comp; i++)
    {
        jpeg_put_bytes(jpeg_buf, (uint8_t [2])
        {
            i + 1, (i == 0) ? 0x00 : 0x11
        }, 2);
    }

    // Spectral selection
    jpeg_put_bytes(jpeg_buf, (uint8_t [3])
    {
        0x00, 0x3F, 0x0
    }, 3);
}

#if 0
/*
 *R0: jpeg_addr        jpeg output buf addr
 *R2: remain_bits_num  Number of remaining empty bits
 *R3: remain_bits      Remaining bits in dword, from msb to lsb
 */
__asm uint8_t *AsmOnlyWriteBits(uint8_t *mem_addr, jpeg_buf_t *cxt, uint32_t bit_num, uint32_t bits)
{
    PUSH    {R4 - R7, LR}
    MOV     R4, R2
    MOV     R5, R3
    LDR     R2, [R1, #0x0]  //Get  remain bits number
    LDR     R3, [R1, #0x4]  //Get  remain bits

    SUBS    R2, R4       //Get new empty bits in dword
    LSL     R6, R5, R2   //Combine bits together, R2: current msb bit position
    ORRGE   R3, R6
    BLT     exit_only_write_bits
    RSB     R2, R2, #0   //Get remain bit number in case negative
    LSR     R6, R5, R2   //Combine bits together
    ORR     R3, R3, R6

    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R6, = 0x01010101
                  UADD8   R7, R3, R6
                  MRS     R7, APSR
                  ANDS    R7, #0xF0000
                  STREQ   R3, [R0], #4 //Write dword if 32bits(Just make aligned write after header )
                  BEQ     exit_calc_write_bits1
                  ANDS    R7, #0x80000 //Write byte
                  LSR     R6, R3, #24
                  STRB    R6, [R0], #1
                  MOV     R6, #0
                  STRBNE  R6, [R0], #1
                  ANDS    R7, #0x40000
                  LSR     R6, R3, #16
                  STRB    R6, [R0], #1
                  MOV     R6, #0
                  STRBNE  R6, [R0], #1
                  ANDS    R5, #0x20000
                  LSR     R6, R3, #8
                  STRB    R6, [R0], #1
                  MOV     R6, #0
                  STRBNE  R6, [R0], #1
                  ANDS    R7, #0x10000
                  LSR     R6, R3, #8
                  STRB    R6, [R0], #1
                  MOV     R6, #0
                  STRBNE  R6, [R0], #1
                  exit_only_write_bits1
                  STR     R2, [R1, #0x0]  //Get  remain bits number
                  STR     R3, [R1, #0x4]  //Get  remain bits
                  LDR     R4, [R1, #0x8]  //Get  buf length
                  ADD     R4, #4
                  STR     R4, [R1, #0x8]  //Put  buf length
                  exit_only_write_bits
                  POP     {R4 - R7, LR}
                  BX      LR
}
/*
 *R0: jpeg_addr        jpeg output buf addr
 *R2: remain_bits_num  Number of remaining empty bits
 *R3: remain_bits      Remaining bits in dword, from msb to lsb
 *R6: coef val
 *R4: bits_num         Number of remaining empty bits
 *R5: valid_bits       Valid bits of one coef
 *R6-R7:               tmp
 */
__asm uint8_t *AsmWriteACBits(void)
{
    //Shall i speed up by making a big table to avoid following code for low-valid-bits coef
    CLZGE   R4, R6
    RSBLT   R5, R6, #0    //Get valid bit number in case negative
    CLZLT   R4, R5
    SUBLT   R6, R6, #1
    AND     R5, R6, 0xFF
    RSB     R4, R4, #32

    LDR     R6, [R1, #12]             //Get addr of Huffman table
    ADD     R6, [R6, R4, #LSL 1]      //Get pos of Huffman table
    LDRD    R6, R7, [R6, R11, #LSL 7] //Get hufmann bit and position(it should be in sram)

    SUBS    R2, R6       //Get new empty bits in dword
    LSL     R6, R7, R2   //Combine bits together, R2: current msb bit position
    ORRGE   R3, R6
    BLT     exit_write_bits
    RSB     R2, R2, #0   //Get remain bit number in case negative
    LSR     R6, R7, R2   //Combine bits together
    ORR     R3, R3, R6

    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R6, [R1, #16] //Get 0x01010101
    UADD8   R7, R3, R6
    MRS     R7, APSR
    ANDS    R7, #0xF0000
    STREQ   R3, [R0], #4 //Write dword if 32bits(Just make aligned write after header )
    BEQ     exit_write_bits1
    ANDS    R7, #0x80000 //Write byte
    LSR     R6, R3, #24
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x40000
    LSR     R6, R3, #16
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R5, #0x20000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x10000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    exit_write_bits1
    MOV     R3, R5       //Left bits
    exit_write_bits

    SUBS    R2, R4       //Get new empty bits in dword
    LSL     R7, R5, R2   //Combine bits together, R2: current msb bit position
    ORRGE   R3, R7
    BLT     exit_write_bits2
    RSB     R2, R2, #0   //Get remain bit number in case negative
    LSR     R7, R5, R2   //Combine bits together
    ORR     R3, R3, R7
    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R6, [R1, #16] //Get 0x01010101
    UADD8   R7, R3, R6
    MRS     R7, APSR
    ANDS    R7, #0xF0000
    STREQ   R3, [R0], #4 //Write dword if 32bits(unaligned write)
    BEQ     exit_write_bits2
    ANDS    R7, #0x80000 //Write byte
    LSR     R6, R3, #24
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x40000
    LSR     R6, R3, #16
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R5, #0x20000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x10000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    exit_write_bits2
    BX      LR
#if 0
    VLDMIA R1!, {S0 - S1}
    VCVT.F32.S16 S0, S0, #0
    VLDMIA R2!, {S8 - S9}
    VCVT.F32.S16 S1, S1, #0
    VMUL.F32  S0, S0, S8
    VCVTR.S32.F32 S0, S0
    VMOV R1, S0
    CMP  R1, #0
    ADDEQ R0, #1
    VSTMIA R2!, {S8 - S9}
#endif
}

/*
 *R0: jpeg_addr        jpeg output buf addr
 *R1:                  Asm decode context
 *R2: remain_bits_num  Number of remaining empty bits
 *R3: remain_bits      Remaining bits in dword, from msb to lsb
 *R8:                  Banks start address
 *R9:                  Bank offset table address
 *R10:                 QT table address
 *R11:                 Zero count
 *R4-R7:               tmp
 *R12:                 Loop count
 */
__asm void *AsmEncodeBlockAC(uint8_t *mem_addr, struct asm_encode_context *cxt, jpeg_buf_t *jpeg_buf)
{
    PUSH    {R4 - R12, LR}
    LDR     R2,  [R2, #0x0]  //Get  remain bits number
    LDR     R3,  [R2, #0x4]  //Get  remain bits
    LDR     R8,  [R1, #0]    //Get bank start addr
    LDR     R9,  [R1, #4]    //Get addr of bank off table
    LDR     R10, [R1, #8]    //Get addr of bank off table

    //First 3 items
    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef
    //Remain 60 items
    block_encode_loop
    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    LDRH    R5,  [R10], #2   //Get QT coef,      1 cycle each coef
    LDRH    R4,  [R9],  #2   //Get bank offset   1 cycle each coef
    LDRH    R4,  [R8, R4, LSL #1] //Get one coef 3 cycle each coef
    SMMULR  R6,  R4, R5      //Quantization and rounding 1 cycle each coef
    ADDEQ   R11, #1          //Zero counting     1 cycle each coef
    BLNE    AsmWriteACBits     //Write this coef   1 cycle each coef

    ADD     R12, #4
    CMP     R12, #64
    BNE     block_encode_loop

    POP     {R4 - R12, LR}
    BX      LR
}
#endif


/*
 *R0: jpeg_addr        jpeg output buf addr
 *R2: remain_bits_num  Number of remaining empty bits
 *R3: remain_bits      Remaining bits in dword, from msb to lsb
 *R6: coef val
 *R4: bits_num         Number of remaining empty bits
 *R5: valid_bits       Valid bits of one coef
 *R6-R7:               tmp
 */
__asm uint8_t *AsmWriteACBits(void)
{
    //Shall i speed up by making a big table to avoid following code for low-valid-bits coef
    CLZGE   R4, R6
    RSBLT   R5, R6, #0    //Get valid bit number in case negative
    CLZLT   R4, R5
    SUBLT   R6, R6, #1
    AND     R5, R6, 0xFF
    RSB     R4, R4, #32

    LDR     R6, [R1, #12]             //Get addr of Huffman table
    LDR     R6, [R6, R4, LSL #1]      //Get pos of Huffman table
    //        LDRD    R6, R7, [R6, R11, LSL #7] //Get hufmann bit and position(it should be in sram)

    SUBS    R2, R6       //Get new empty bits in dword
    LSL     R6, R7, R2   //Combine bits together, R2: current msb bit position
    ORRGE   R3, R6
    BLT     exit_write_bits
    RSB     R2, R2, #0   //Get remain bit number in case negative
    LSR     R6, R7, R2   //Combine bits together
    ORR     R3, R3, R6

    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R6, [R1, #16] //Get 0x01010101
    UADD8   R7, R3, R6
    MRS     R7, APSR
    ANDS    R7, #0xF0000
    STREQ   R3, [R0], #4 //Write dword if 32bits(Just make aligned write after header )
    BEQ     exit_write_bits1
    ANDS    R7, #0x80000 //Write byte
    LSR     R6, R3, #24
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x40000
    LSR     R6, R3, #16
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R5, #0x20000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x10000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    exit_write_bits1
    MOV     R3, R5       //Left bits
    exit_write_bits

    SUBS    R2, R4       //Get new empty bits in dword
    LSL     R7, R5, R2   //Combine bits together, R2: current msb bit position
    ORRGE   R3, R7
    BLT     exit_write_bits2
    RSB     R2, R2, #0   //Get remain bit number in case negative
    LSR     R7, R5, R2   //Combine bits together
    ORR     R3, R3, R7
    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R6, [R1, #16] //Get 0x01010101
    UADD8   R7, R3, R6
    MRS     R7, APSR
    ANDS    R7, #0xF0000
    STREQ   R3, [R0], #4 //Write dword if 32bits(unaligned write)
    BEQ     exit_write_bits2
    ANDS    R7, #0x80000 //Write byte
    LSR     R6, R3, #24
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x40000
    LSR     R6, R3, #16
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R5, #0x20000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    ANDS    R7, #0x10000
    LSR     R6, R3, #8
    STRB    R6, [R0], #1
    MOV     R6, #0
    STRBNE  R6, [R0], #1
    exit_write_bits2
    BX      LR
}
/*
 *R0: jpeg_addr        jpeg output buf addr
 *R1:                  Bank output addr
 *R2: remain_bits_num  Number of remaining empty bits
 *R3: remain_bits      Remaining bits in dword, starting from bit 31
 *R4:                  Dct data in one block
 *R5:                  Remain bits in one block
 *R6:                  Last none-zero position in one block
 *R7:                  Zero bytes
 *R8-R9:               tmp
 *R10:                 Mixed table address
 *R11:                 Rounding constant
 *R12:                 Decode context
 *jpeg_encode_context
 *bitc                 #0
 *bitb                 #4
 *row                  #8
 *col_num              #12
 *start_block          #16
 *end_block            #20
 *block_idx            #24
 *table_idx            #28
 *table_len            #32
 *col                  #36
 *ch_size              #40
 *mt[MAX_TABLE_NUM]    #44
 *DC[MAX_TABLE_NUM]    #56
 *constanst            #68
 */

__asm uint8_t *AsmPutBlock(uint8_t *mem_addr, uint8_t *bank_addr, struct jpeg_encode_context *decode_cxt)
{
    PUSH    {R4 - R12, LR}
    MOV     R12,  R2
    LDR     R2, [R12, #0]
    LDR     R3, [R12, #4]

#ifdef ASM_PUT_BLOCK_SHORT_LOOP
    loop_encode_block
    //Get address of MT by table_idx(0x28 offset)
    LDR     R9,  [R12, #28]
    ADD     R10, R12, #44
    LDR     R10, [R10, R9, LSL #2]

    //Write DC bits
    LDR     R7, [R10, #0x440]    //Get Bank offset
    LDRSH   R4, [R1, R7]         //Get DC bits
    LDR     R7, [R10, #0x444]    //Get QT coef
    SMMULR  R4,  R7,  R4
    ADD     R11, R12, #56       //Get last DC
    LDR     R5,  [R11, R9, LSL #2]
    STR     R4,  [R11, R9, LSL #2] //Save this DC
    SUBS    R4,  R5
    BNE     jpeg_write_dc_bits
    LDR     R8, [R10, #0x400] //Write default dc bits R4:dc bits R7:Number of dc bits
    ASR     R5, R8, #16
    RSB     R4, R5, #32   //R5: Number of valid bits
    LSL     R4, R8, R4
    BL      jpeg_write_bits
    B       process_block_ac
    jpeg_write_dc_bits
    //Calculate bits
    CLZGE   R5, R4
    RSBLT   R5, R4, #0    //Get valid bit number in case negative
    CLZLT   R5, R5
    SUBLT   R4, R4, #1
    LSL     R4, R4, R5    //Clear leading zeros, valid msb bit on bit 31
    RSB     R5, R5, #32   //R5: Number of valid bits
    ADD     R11, R10, #0x400
    LDR     R8, [R11, R5, LSL #2] //Get hufmann bit and position(it should be in sram) R8
    BL  jpeg_write_two_bits

    process_block_ac
    ADD     R5, R10, #0x448
    MOV     R11, #1
    MOV     R6,  #1       //First zero counting position

    loop_encode_block_ac
    LDRD    R7,  R8, [R5, #0]
    LDRSH   R4,  [R1, R7]
    SMMULR  R4,  R8,  R4
    CMP     R4,  #0
    BLNE    encode_block_ac_zero_count0
    LDRD    R7,  R8, [R5, #8]
    LDRSH   R4,  [R1, R7]
    SMMULR  R4,  R8,  R4
    CMP     R4,  #0
    BLNE    encode_block_ac_zero_count1
    LDRD    R7,  R8, [R5, #16]
    LDRSH   R4,  [R1, R7]
    SMMULR  R4,  R8,  R4
    CMP     R4,  #0
    BLNE    encode_block_ac_zero_count2
    ADD     R11, #3
    ADD     R5,  #24
    CMP     R11, #64
    BLT     loop_encode_block_ac

    CMP     R6,  #64
    BGE     exit_write_eob_bits
    LDR     R8, [R10, #0] //Get 0xF0 item
    ADD     R10, #2
    B       jpeg_write_two_bits
    exit_write_eob_bits
    //Process all yuv tables
    ADD     R1, #8           //Increase bank address
    LDR     R9, [R12, #28]   //Get table_index
    ADD     R9, #1
    LDR     R8, [R12, #32]   //Get table_len
    CMP     R9, R8
    STRLT   R9, [R12, #28]
    BLT     loop_encode_block
    //Process all blocks
    LDR     R9, [R12, #24]   //Increase block index
    ADD     R9, #1
    LDR     R8, [R12, #20]
    CMP     R9, R8
    BGE.W   exit_put_block
    STR     R9, [R12, #24]

    MOV     R9, #0           //Reset table index
    STR     R9, [R12, #28]
    LDR     R9, [R12, #36]
    ADD     R9, #6
    LDR     R8, [R12, #12]   //Get col_num
    CMP     R9, R8
    STRLT   R9, [R12, #36]
    BLT     loop_encode_block
    MOV     R9, #0           //Reset bank_col
    STR     R9, [R12, #36]

    LDR     R9, [R12, #8]
    ADD     R9, #4
    STR     R9, [R12, #8]
    ADD     R1, R1, #0x8000       //Increase One Bank
    ANDS    R9, R9, #0x7
    SUBEQ   R1, #0x10000          //Go back to Bank 0
    SUBNE   R1, R1, R8,  LSL #2   //Bank pointer add one col (+ len_col)
    B       loop_encode_block

    encode_block_ac_zero_count0
    SUB     R7,  R11, R6      //Zero counting
    ADD     R6,  R11, #1
    B       jpeg_data_put_stream
    encode_block_ac_zero_count1
    ADD     R7,  R11, #1
    SUB     R7,  R7,  R6      //Zero counting
    ADD     R6,  R11, #2
    B       jpeg_data_put_stream
    encode_block_ac_zero_count2
    ADD     R7,  R11, #2
    SUB     R7,  R7,  R6      //Zero counting
    ADD     R6,  R11, #3
    // B       jpeg_data_put_stream

#else
    loop_encode_block
    //Get all block data into FPU register
    MOV     R8,  R1
    ADD     R9,  R1,  #0x4000
    ADD     R10, R1,  #0x10000
    ADD     R11, R1,  #0x14000
    LDR     R5,  [R12, #40]

    VLDR.64 D0, [R8,  #0x0000]
    VLDR.64 D1, [R9,  #0x0000]
    VLDR.64 D2, [R10, #0x0000]
    VLDR.64 D3, [R11, #0x0000]
    ADD  R8,  R5//0x03f0
    ADD  R9,  R5//0x43f0
    ADD  R10, R5//0x103f0
    ADD  R11, R5//0x143f0
    VLDR.64 D4, [R8,  #0x0000]
    VLDR.64 D5, [R9,  #0x0000]
    VLDR.64 D6, [R10, #0x0000]
    VLDR.64 D7, [R11, #0x0000]
    ADD  R8,  R5//0x07e0
    ADD  R9,  R5//0x47e0
    ADD  R10, R5//0x107e0
    ADD  R11, R5//0x147e0
    VLDR.64 D8, [R8,  #0x0000]
    VLDR.64 D9, [R9,  #0x0000]
    VLDR.64 D10, [R10, #0x0000]
    VLDR.64 D11, [R11, #0x0000]
    ADD  R8,  R5//0x0bd0
    ADD  R9,  R5//0x4bd0
    ADD  R10, R5//0x10bd0
    ADD  R11, R5//0x14bd0
    VLDR.64 D12, [R8,  #0x0000]
    VLDR.64 D13, [R9,  #0x0000]
    VLDR.64 D14, [R10, #0x0000]
    VLDR.64 D15, [R11, #0x0000]


    //Get address of MT by table_idx(0x28 offset)
    LDR     R9,  [R12, #28]
    ADD     R10, R12, #44
    LDR     R10, [R10, R9, LSL #2]

    //Write DC bits
    VMOV    R4, S0            //Get DC bits
    LDR     R7, [R10, #0x440] //Get QT coef
    MOV     R11, #0x800       //Rounding
    SMLAWT  R4,  R7, R4, R11
    ASRS    R4,  R4, #12
    ADD     R11, R12, #56       //Get last DC
    LDR     R5,  [R11, R9, LSL #2]
    STR     R4,  [R11, R9, LSL #2] //Save this DC
    SUBS    R4,  R5
    BNE     jpeg_write_dc_bits
    LDR     R8, [R10, #0x400] //Write default dc bits R4:dc bits R7:Number of dc bits
    ASR     R5, R8, #16
    RSB     R4, R5, #32   //R5: Number of valid bits
    LSL     R4, R8, R4
    BL      jpeg_write_bits
    B       process_block_ac
    jpeg_write_dc_bits
    //Calculate bits
    CLZGE   R5, R4
    RSBLT   R5, R4, #0    //Get valid bit number in case negative
    CLZLT   R5, R5
    SUBLT   R4, R4, #1
    LSL     R4, R4, R5    //Clear leading zeros, valid msb bit on bit 31
    RSB     R5, R5, #32   //R5: Number of valid bits
    ADD     R11, R10, #0x400
    LDR     R8, [R11, R5, LSL #2] //Get hufmann bit and position(it should be in sram) R8
    BL  jpeg_write_two_bits
    process_block_ac
    /*
       uint16_t zigzag_bank_off[64] = {
    C0C0     C0C1     C1C0     C2C0     C1C1     C0C2     C0C3     C1C2
    S0T      S1T      S4T      S8T      S5T      S0B      S1B      S4B
    0x2,     0x6,     0x10002, 0x3f2,   0x10006, 0x0,     0x4,     0x10000,
    C2C1     C3C0     C4C0     C3C1     C2C2     C1C3     C0C4     C0C5
    S9T      S12T     S16T     S13T      S8B      S5B      S2T     S3T
    0x3f6,   0x103f2, 0x7e2,   0x103f6, 0x3f0,   0x10004, 0x4002,  0x4006,
    C1C4     C2C3     C3C2     C4C1     C5C0     C6C0     C5C1     C4C2
    S6T      S9B      S12B     S17T     S20T     S24T     S21T     S16B
    0x14002, 0x3f4,   0x103f0, 0x7e6,   0x107e2, 0xbd2,   0x107e6, 0x7e0,
    C3C3     C2C4     C1C5     C0C6     C0C7     C1C6     C2C5     C3C4
    S13B     S10T     S7T      S2B      S3B      S6B      S11T     S14T
    0x103f4, 0x43f2,  0x14006, 0x4000,  0x4004,  0x14000, 0x43f6,  0x143f2,
    C4C3     C5C2     C6C1     C7C0     C7C1     C6C2     C5C3     C4C4
    S17B     S20B     S25T     S28T     S29T     S24B     S21B     S18T
    0x7e4,   0x107e0, 0xbd6,   0x10bd2, 0x10bd6, 0xbd0,   0x107e4, 0x47e2,
    C3C5     C2C6     C1C7     C2C7     C3C6     C4C5     C5C4     C6C3
    S15T     S10B     S7B      S11B     S14B     S19T     S22T     S25B
    0x143f6, 0x43f0,  0x14004, 0x43f4,  0x143f0, 0x47e6,  0x147e2, 0xbd4,
    C7C2     C7C3     C6C4     C5C5     C4C6     C3C7     C4C7     C5C6
    S28B     S29B     S26T     S23T     S18B     S15B     S19B     S22B
    0x10bd0, 0x10bd4, 0x4bd2,  0x147e6, 0x47e0,  0x143f4, 0x47e4,  0x147e0,
    C6C5     C7C4     C7C5     C6C6     C5C7     C6C7     C7C6     C7C7
    S27T     S30T     S31T     S26B     S23B     S27B     S30B     S31B
    0x4bd6,  0x14bd2, 0x14bd6, 0x4bd0,  0x147e4, 0x4bd4,  0x14bd0, 0x14bd4,
    };
    */

    //S0T      S1T      S4T      S8T      S5T      S0B      S1B      S4B
    MOV     R11, #0x800     //Rounding
    //LSR     R11, R11, #16 //0x0000FFFF
    MOV     R6,  #1       //First zero counting position

    //S0T      S1T      S4T      S8T      S5T      S0B      S1B      S4B
    VMOV    R4, S1  //S1T 1
    LDR     R7,  [R10, #0x444] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag1_put_stream
    VMOV    R4, S4   //S4T 8
    LDR     R7,  [R10, #0x448] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag2_put_stream
    VMOV    R4, S8  //S8T 16
    LDR     R7,  [R10, #0x44c] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag3_put_stream
    VMOV    R4, S5   //S5T 9
    LDR     R7,  [R10, #0x450] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag4_put_stream
    VMOV    R4, S0  //S0B 2
    LDR     R7,  [R10, #0x454] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag5_put_stream
    VMOV    R4, S1  //S1B 3
    LDR     R7,  [R10, #0x458] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag6_put_stream
    VMOV    R4, S4  //S4B 10
    LDR     R7,  [R10, #0x45c] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag7_put_stream
    //S9T      S12T     S16T     S13T      S8B      S5B      S2T     S3T
    VMOV    R4, S9   //S9T 17
    LDR     R7,  [R10, #0x460] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag8_put_stream
    VMOV    R4, S12  //S12T 24
    LDR     R7,  [R10, #0x464] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag9_put_stream
    VMOV    R4, S16  //S16T 32
    LDR     R7,  [R10, #0x468] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag10_put_stream
    VMOV    R4, S13  //S13T 25
    LDR     R7,  [R10, #0x46c] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag11_put_stream
    VMOV    R4, S8   //S8B 18
    LDR     R7,  [R10, #0x470] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag12_put_stream
    VMOV    R4, S5   //S5B 11
    LDR     R7,  [R10, #0x474] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag13_put_stream
    VMOV    R4, S2   //S2T 4
    LDR     R7,  [R10, #0x478] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag14_put_stream
    VMOV    R4, S3   //S3T 5
    LDR     R7,  [R10, #0x47c] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag15_put_stream
    //S6T      S9B      S12B     S17T     S20T     S24T     S21T     S16B
    VMOV    R4, S6   //S6T 12
    LDR     R7,  [R10, #0x480] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag16_put_stream
    VMOV    R4, S9  //S9B 19
    LDR     R7,  [R10, #0x484] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag17_put_stream
    VMOV    R4, S12   //S12B 26
    LDR     R7,  [R10, #0x488] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag18_put_stream
    VMOV    R4, S17  //S17T 33
    LDR     R7,  [R10, #0x48c] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag19_put_stream
    VMOV    R4, S20   //S20T 40
    LDR     R7,  [R10, #0x490] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag20_put_stream
    VMOV    R4, S24  //S24T 48
    LDR     R7,  [R10, #0x494] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag21_put_stream
    VMOV    R4, S21  //S21T 41
    LDR     R7,  [R10, #0x498] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag22_put_stream
    VMOV    R4, S16  //S16B 34
    LDR     R7,  [R10, #0x49c] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag23_put_stream
    //S13B     S10T     S7T      S2B      S3B      S6B      S11T     S14T
    VMOV    R4, S13   //S13B 27
    LDR     R7,  [R10, #0x4a0] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag24_put_stream
    VMOV    R4, S10  //S10T 20
    LDR     R7,  [R10, #0x4a4] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag25_put_stream
    VMOV    R4, S7  //S7T 13
    LDR     R7,  [R10, #0x4a8] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag26_put_stream
    VMOV    R4, S2  //S2B 6
    LDR     R7,  [R10, #0x4ac] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag27_put_stream
    VMOV    R4, S3   //S3B 7
    LDR     R7,  [R10, #0x4b0] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag28_put_stream
    VMOV    R4, S6   //S6B 14
    LDR     R7,  [R10, #0x4b4] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag29_put_stream
    VMOV    R4, S11   //S11T 21
    LDR     R7,  [R10, #0x4b8] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag30_put_stream
    VMOV    R4, S14   //S14T 28
    LDR     R7,  [R10, #0x4bc] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag31_put_stream
    //S17B     S20B     S25T     S28T     S29T     S24B     S21B      S18T
    VMOV    R4, S17   //S17B 35
    LDR     R7,  [R10, #0x4c0] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag32_put_stream
    VMOV    R4, S20  //S20B 42
    LDR     R7,  [R10, #0x4c4] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag33_put_stream
    VMOV    R4, S25  //S25T 49
    LDR     R7,  [R10, #0x4c8] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag34_put_stream
    VMOV    R4, S28  //S28T 56
    LDR     R7,  [R10, #0x4cc] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag35_put_stream
    VMOV    R4, S29  //S29T 57
    LDR     R7,  [R10, #0x4d0] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag36_put_stream
    VMOV    R4, S24   //S24B 50
    LDR     R7,  [R10, #0x4d4] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag37_put_stream
    VMOV    R4, S21   //S21B 43
    LDR     R7,  [R10, #0x4d8] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag38_put_stream
    VMOV    R4, S18   //S18T 36
    LDR     R7,  [R10, #0x4dc] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag39_put_stream

    //S15T     S10B     S7B      S11B     S14B     S19T     S22T     S25B
    VMOV    R4, S15   //S15T 29
    LDR     R7,  [R10, #0x4e0] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag40_put_stream
    VMOV    R4, S10  //S10B 22
    LDR     R7,  [R10, #0x4e4] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag41_put_stream
    VMOV    R4, S7  //S7B 15
    LDR     R7,  [R10, #0x4e8] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag42_put_stream
    VMOV    R4, S11  //S11B 23
    LDR     R7,  [R10, #0x4ec] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag43_put_stream
    VMOV    R4, S14  //S14B 30
    LDR     R7,  [R10, #0x4f0] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag44_put_stream
    VMOV    R4, S19   //S19T 37
    LDR     R7,  [R10, #0x4f4] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag45_put_stream
    VMOV    R4, S22   //S22T 44
    LDR     R7,  [R10, #0x4f8] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag46_put_stream
    VMOV    R4, S25   //S25B 51
    LDR     R7,  [R10, #0x4fc] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag47_put_stream
    //S28B     S29B     S26T     S23T     S18B     S15B     S19B     S22B
    VMOV    R4, S28   //S28B 58
    LDR     R7,  [R10, #0x500] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag48_put_stream
    VMOV    R4, S29  //S29B 59
    LDR     R7,  [R10, #0x504] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag49_put_stream
    VMOV    R4, S26  //S26T 52
    LDR     R7,  [R10, #0x508] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag50_put_stream
    VMOV    R4, S23  //S23T 45
    LDR     R7,  [R10, #0x50c] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag51_put_stream
    VMOV    R4, S18  //S18B 38
    LDR     R7,  [R10, #0x510] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag52_put_stream
    VMOV    R4, S15   //S15B 31
    LDR     R7,  [R10, #0x514] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag53_put_stream
    VMOV    R4, S19   //S19B 39
    LDR     R7,  [R10, #0x518] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag54_put_stream
    VMOV    R4, S22   //S22B 46
    LDR     R7,  [R10, #0x51c] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag55_put_stream
    //S27T     S30T     S31T     S26B     S23B     S27B     S30B     S31B
    VMOV    R4, S27   //S27T 53
    LDR     R7,  [R10, #0x520] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag56_put_stream
    VMOV    R4, S30  //S30T 60
    LDR     R7,  [R10, #0x524] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag57_put_stream
    VMOV    R4, S31  //S31T 61
    LDR     R7,  [R10, #0x528] //Get QT coef,      1 cycle each coef
    SMLAWT  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag58_put_stream
    VMOV    R4, S26  //S26B 54
    LDR     R7,  [R10, #0x52c] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag59_put_stream
    VMOV    R4, S23  //S23B 47
    LDR     R7,  [R10, #0x530] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag60_put_stream
    VMOV    R4, S27   //S27B 55
    LDR     R7,  [R10, #0x534] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag61_put_stream
    VMOV    R4, S30   //S30B 62
    LDR     R7,  [R10, #0x538] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag62_put_stream
    VMOV    R4, S31   //S31B 63
    LDR     R7,  [R10, #0x53c] //Get QT coef,      1 cycle each coef
    SMLAWB  R4,  R7, R4, R11   //Quantization and rounding 1 cycle each coef
    ASRS    R4,  R4, #12
    BLNE    zigzag63_put_stream

    CMP     R6,  #64
    BGE     exit_write_eob_bits
    LDR     R8, [R10, #0] //Get 0xF0 item
    ADD     R10, #2
    B       jpeg_write_two_bits
    exit_write_eob_bits
    //Process all yuv tables
    ADD     R1, #8           //Increase bank address
    LDR     R9, [R12, #28]   //Get table_index
    ADD     R9, #1
    LDR     R8, [R12, #32]   //Get table_len
    CMP     R9, R8
    STRLT   R9, [R12, #28]
    BLT     loop_encode_block
    //Process all blocks
    LDR     R9, [R12, #24]   //Increase block index
    ADD     R9, #1
    LDR     R8, [R12, #20]
    CMP     R9, R8
    BGE.W   exit_put_block
    STR     R9, [R12, #24]

    MOV     R9, #0           //Reset table index
    STR     R9, [R12, #28]
    LDR     R9, [R12, #36]
    ADD     R9, #6
    LDR     R8, [R12, #12]   //Get col_num
    CMP     R9, R8
    STRLT   R9, [R12, #36]
    BLT     loop_encode_block
    MOV     R9, #0           //Reset bank_col
    STR     R9, [R12, #36]

    LDR     R9, [R12, #8]
    ADD     R9, #4
    STR     R9, [R12, #8]
    ADD     R1, R1, #0x8000       //Increase One Bank
    ANDS    R9, R9, #0x7
    SUBEQ   R1, #0x10000          //Go back to Bank 0
    SUBNE   R1, R1, R8,  LSL #2   //Bank pointer add one col (+ len_col)
    B       loop_encode_block
    //S0T      S1T      S4T      S8T      S5T      S0B      S1B      S4B
    zigzag1_put_stream
    RSB     R7,  R6, #1        //Zero counting
    MOV     R6,  #2
    B       jpeg_data_put_stream
    zigzag2_put_stream
    RSB     R7,  R6, #2        //Zero counting
    MOV     R6,  #3
    B       jpeg_data_put_stream
    zigzag3_put_stream
    RSB     R7,  R6, #3        //Zero counting
    MOV     R6,  #4
    B       jpeg_data_put_stream
    zigzag4_put_stream
    RSB     R7,  R6, #4        //Zero counting
    MOV     R6,  #5
    B       jpeg_data_put_stream
    zigzag5_put_stream
    RSB     R7,  R6, #5        //Zero counting
    MOV     R6,  #6
    B       jpeg_data_put_stream
    zigzag6_put_stream
    RSB     R7,  R6, #6        //Zero counting
    MOV     R6,  #7
    B       jpeg_data_put_stream
    zigzag7_put_stream
    RSB     R7,  R6, #7        //Zero counting
    MOV     R6,  #8
    B       jpeg_data_put_stream
    //S9T      S12T     S16T     S13T      S8B      S5B      S2T     S3T
    zigzag8_put_stream
    RSB     R7,  R6, #8        //Zero counting
    MOV     R6,  #9
    B       jpeg_data_put_stream
    zigzag9_put_stream
    RSB     R7,  R6, #9        //Zero counting
    MOV     R6,  #10
    B       jpeg_data_put_stream
    zigzag10_put_stream
    RSB     R7,  R6, #10       //Zero counting
    MOV     R6,  #11
    B       jpeg_data_put_stream
    zigzag11_put_stream
    RSB     R7,  R6, #11       //Zero counting
    MOV     R6,  #12
    B       jpeg_data_put_stream
    zigzag12_put_stream
    RSB     R7,  R6, #12       //Zero counting
    MOV     R6,  #13
    B       jpeg_data_put_stream
    zigzag13_put_stream
    RSB     R7,  R6, #13       //Zero counting
    MOV     R6,  #14
    B       jpeg_data_put_stream
    zigzag14_put_stream
    RSB     R7,  R6, #14       //Zero counting
    MOV     R6,  #15
    B       jpeg_data_put_stream
    zigzag15_put_stream
    RSB     R7,  R6, #15       //Zero counting
    MOV     R6,  #16
    B       jpeg_data_put_stream
    //S6T      S9B      S12B     S17T     S20T     S24T     S21T     S16B
    zigzag16_put_stream
    RSB     R7,  R6, #16       //Zero counting
    MOV     R6,  #17
    B       jpeg_data_put_stream
    zigzag17_put_stream
    RSB     R7,  R6, #17       //Zero counting
    MOV     R6,  #18
    B       jpeg_data_put_stream
    zigzag18_put_stream
    RSB     R7,  R6, #18       //Zero counting
    MOV     R6,  #19
    B       jpeg_data_put_stream
    zigzag19_put_stream
    RSB     R7,  R6, #19       //Zero counting
    MOV     R6,  #20
    B       jpeg_data_put_stream
    zigzag20_put_stream
    RSB     R7,  R6, #20       //Zero counting
    MOV     R6,  #21
    B       jpeg_data_put_stream
    zigzag21_put_stream
    RSB     R7,  R6, #21       //Zero counting
    MOV     R6,  #22
    B       jpeg_data_put_stream
    zigzag22_put_stream
    RSB     R7,  R6, #22       //Zero counting
    MOV     R6,  #23
    B       jpeg_data_put_stream
    zigzag23_put_stream
    RSB     R7,  R6, #23       //Zero counting
    MOV     R6,  #24
    B       jpeg_data_put_stream
    //S13B     S10T     S7T      S2B      S3B      S6B      S11T     S14T
    zigzag24_put_stream
    RSB     R7,  R6, #24       //Zero counting
    MOV     R6,  #25
    B       jpeg_data_put_stream
    zigzag25_put_stream
    RSB     R7,  R6, #25       //Zero counting
    MOV     R6,  #26
    B       jpeg_data_put_stream
    zigzag26_put_stream
    RSB     R7,  R6, #26       //Zero counting
    MOV     R6,  #27
    B       jpeg_data_put_stream
    zigzag27_put_stream
    RSB     R7,  R6, #27       //Zero counting
    MOV     R6,  #28
    B       jpeg_data_put_stream
    zigzag28_put_stream
    RSB     R7,  R6, #28       //Zero counting
    MOV     R6,  #29
    B       jpeg_data_put_stream
    zigzag29_put_stream
    RSB     R7,  R6, #29       //Zero counting
    MOV     R6,  #30
    B       jpeg_data_put_stream
    zigzag30_put_stream
    RSB     R7,  R6, #30       //Zero counting
    MOV     R6,  #31
    B       jpeg_data_put_stream
    zigzag31_put_stream
    RSB     R7,  R6, #31       //Zero counting
    MOV     R6,  #32
    B       jpeg_data_put_stream
    //S17B     S20B     S25T     S28T     S29T     S24B     S21B      S18T
    zigzag32_put_stream
    RSB     R7,  R6, #32      //Zero counting
    MOV     R6,  #33
    B       jpeg_data_put_stream
    zigzag33_put_stream
    RSB     R7,  R6, #33      //Zero counting
    MOV     R6,  #34
    B       jpeg_data_put_stream
    zigzag34_put_stream
    RSB     R7,  R6, #34      //Zero counting
    MOV     R6,  #35
    B       jpeg_data_put_stream
    zigzag35_put_stream
    RSB     R7,  R6, #35      //Zero counting
    MOV     R6,  #36
    B       jpeg_data_put_stream
    zigzag36_put_stream
    RSB     R7,  R6, #36      //Zero counting
    MOV     R6,  #37
    B       jpeg_data_put_stream
    zigzag37_put_stream
    RSB     R7,  R6, #37      //Zero counting
    MOV     R6,  #38
    B       jpeg_data_put_stream
    zigzag38_put_stream
    RSB     R7,  R6, #38      //Zero counting
    MOV     R6,  #39
    B       jpeg_data_put_stream
    zigzag39_put_stream
    RSB     R7,  R6, #39      //Zero counting
    MOV     R6,  #40
    B       jpeg_data_put_stream
    //S15T     S10B     S7B      S11B     S14B     S19T     S22T     S25B
    zigzag40_put_stream
    RSB     R7,  R6, #40      //Zero counting
    MOV     R6,  #41
    B       jpeg_data_put_stream
    zigzag41_put_stream
    RSB     R7,  R6, #41      //Zero counting
    MOV     R6,  #42
    B       jpeg_data_put_stream
    zigzag42_put_stream
    RSB     R7,  R6, #42      //Zero counting
    MOV     R6,  #43
    B       jpeg_data_put_stream
    zigzag43_put_stream
    RSB     R7,  R6, #43      //Zero counting
    MOV     R6,  #44
    B       jpeg_data_put_stream
    zigzag44_put_stream
    RSB     R7,  R6, #44      //Zero counting
    MOV     R6,  #45
    B       jpeg_data_put_stream
    zigzag45_put_stream
    RSB     R7,  R6, #45      //Zero counting
    MOV     R6,  #46
    B       jpeg_data_put_stream
    zigzag46_put_stream
    RSB     R7,  R6, #46      //Zero counting
    MOV     R6,  #47
    B       jpeg_data_put_stream
    zigzag47_put_stream
    RSB     R7,  R6, #47      //Zero counting
    MOV     R6,  #48
    B       jpeg_data_put_stream
    //S28B     S29B     S26T     S23T     S18B     S15B     S19B     S22B
    zigzag48_put_stream
    RSB     R7,  R6, #48      //Zero counting
    MOV     R6,  #49
    B       jpeg_data_put_stream
    zigzag49_put_stream
    RSB     R7,  R6, #49      //Zero counting
    MOV     R6,  #50
    B       jpeg_data_put_stream
    zigzag50_put_stream
    RSB     R7,  R6, #50      //Zero counting
    MOV     R6,  #51
    B       jpeg_data_put_stream
    zigzag51_put_stream
    RSB     R7,  R6, #51      //Zero counting
    MOV     R6,  #52
    B       jpeg_data_put_stream
    zigzag52_put_stream
    RSB     R7,  R6, #52      //Zero counting
    MOV     R6,  #53
    B       jpeg_data_put_stream
    zigzag53_put_stream
    RSB     R7,  R6, #53      //Zero counting
    MOV     R6,  #54
    B       jpeg_data_put_stream
    zigzag54_put_stream
    RSB     R7,  R6, #54      //Zero counting
    MOV     R6,  #55
    B       jpeg_data_put_stream
    zigzag55_put_stream
    RSB     R7,  R6, #55      //Zero counting
    MOV     R6,  #56
    B       jpeg_data_put_stream
    //S27T     S30T     S31T     S26B     S23B     S27B     S30B     S31B
    zigzag56_put_stream
    RSB     R7,  R6, #56      //Zero counting
    MOV     R6,  #57
    B       jpeg_data_put_stream
    zigzag57_put_stream
    RSB     R7,  R6, #57      //Zero counting
    MOV     R6,  #58
    B       jpeg_data_put_stream
    zigzag58_put_stream
    RSB     R7,  R6, #58      //Zero counting
    MOV     R6,  #59
    B       jpeg_data_put_stream
    zigzag59_put_stream
    RSB     R7,  R6, #59      //Zero counting
    MOV     R6,  #60
    B       jpeg_data_put_stream
    zigzag60_put_stream
    RSB     R7,  R6, #60      //Zero counting
    MOV     R6,  #61
    B       jpeg_data_put_stream
    zigzag61_put_stream
    RSB     R7,  R6, #61      //Zero counting
    MOV     R6,  #62
    B       jpeg_data_put_stream
    zigzag62_put_stream
    RSB     R7,  R6, #62      //Zero counting
    MOV     R6,  #63
    B       jpeg_data_put_stream
    zigzag63_put_stream
    RSB     R7,  R6, #63     //Zero counting
    MOV     R6,  #64          //Go back to first value 1
    //B       jpeg_data_put_stream
#endif
    jpeg_data_put_stream
    //calculate_valid_bits
    //Calculate bits
    CLZGE   R5, R4
    RSBLT   R5, R4, #0    //Get valid bit number in case negative
    CLZLT   R5, R5
    SUBLT   R4, R4, #1
    LSL     R4, R4, R5    //Clear leading zeros, valid msb bit on bit 31
    RSB     R5, R5, #32   //R5: Number of valid bits
    //Write zero-count beyond 16
    write_16_zero_bits
    CMP     R7, #16
    BLT     write_huffman_bits
    LDR     R8, [R10, #0x3C0] //Get 0xF0 item
    ADD     R10, #1
    SUB     R7, R7, #16
    B       jpeg_write_two_bits //Never ruin R7, because zero > 16
    write_huffman_bits
    //Get Huffman bits
    ADD     R8, R5, R7, LSL #4    //Get pos of Huffman table
    LDR     R8, [R10, R8, LSL #2] //Get hufmann bit and position(it should be in sram)

    jpeg_write_two_bits
    SUBS    R2, R2, R8, LSR #16  //Get new empty bits in dword
    UXTH    R8
    LSLGE   R9, R8, R2           //Combine huffman bits, R2: current msb bit position
    ORRGE   R3, R9
    BGE     exit_jpeg_write_two_bits
    RSB     R9, R2, #0          //Get remain bit number in case negative
    LSR     R9, R8, R9          //Combine huffman bits together
    ORR     R3, R3, R9
    ADD     R2, R2, #32         //Get number of remain empty bits
#ifdef ASM_PUT_BLOCK_SHORT_LOOP
    VMOV    S0, R11
#endif
    LSL     R11, R8, R2         //Get remain bits R11
    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R8, [R12, #68] //Get 0x01010101
    UADD8   R8, R3, R8
    MRS     R8, APSR
    ANDS    R8, #0xF0000
    REVEQ   R3, R3
    STREQ   R3, [R0], #4 //Write dword if 32bits(Just make aligned write after header )
    MOVEQ   R3, R11
    BEQ     exit_jpeg_write_two_bits1
    MOV     R9, #32
    loop_write_padding_bytes
    ROR     R3, R3, #24     //Shift to bit 0, remain bit at bit 31
    STRB    R3, [R0], #1    //Write valid byte
    SXTB    R8, R3
    ADDS    R8, #1
    STRBEQ  R8, [R0], #1    //Write padding in case 0xFF
    SUB     R9, #8
    ANDS    R8, R0, #3      //Check aligned address
    LSL     R8, #3
    ADD     R8, R9
    CMP     R8, #32         //Continue padding if aligned dword is not arrived
    BGE     loop_write_padding_bytes
    RSB     R8, R9, #32
    LSR     R3, R3, R8     //Clear empty bits to zero
    LSL     R3, R3, R8
    ANDS    R8, R0, #1      //Check additional 0x00
    SUB     R0, R8
    LSL     R8, #3
    ADD     R9, R8          //Remain bits after one dword padding
    LSRNE   R3, R3, #8      //Insert additional 0x00
    LSR     R11, R11, R9    //R3 get remain bits
    ORR     R3, R11
    SUB     R2, R9         //Get number of remain empty bits
    exit_jpeg_write_two_bits1      //Resume R11, I can't use R7, so R11 is only choice
#ifdef ASM_PUT_BLOCK_SHORT_LOOP
    VMOV    R11, S0
#else
    MOV     R11, #0x800
#endif
    exit_jpeg_write_two_bits          //Special handling of return address
    ANDS    R8, R10, #3
    BEQ     jpeg_write_bits
    SUB     R10, R10, #1
    ANDS    R8, R10, #3
    BEQ     write_16_zero_bits
    SUB     R10, R10, #1
    ANDS    R8, R10, #3
    BEQ     exit_write_eob_bits
    ADD     R5, R10, #0x440
    ADD     R5, R11, LSL #3
    BX      LR
    jpeg_write_bits  //R4 valid bits(msb on 31 bit); R5 number of valid bits; R3 valid bits; R2 empty bits of R3; R7 tmp
    RSB     R8, R2, #32
    LSR     R8, R4, R8          //Combine huffman bits, R2: current msb bit position
    ORR     R3, R8
    SUBS    R2, R2, R5          //Get new empty bits in dword
    ADDGE   R5, R10, #0x440
    ADDGE   R5, R11, LSL #3
    BXGE    LR
    ADD     R5, R2, R5          //Get original empty bits in negative case
    LSL     R5, R4, R5          //Get remain valid bits, msb on 31 bit
    ADD     R2, R2, #32         //Get remain empty bits number in case negative
    //Save one dword with padding
    //Shall I use unaligned dword write in case that no padding is necessary
    LDR     R9, [R12, #68] //Get 0x01010101
    UADD8   R8, R3, R9
    MRS     R8, APSR
    ANDS    R8, #0xF0000
    REVEQ   R3, R3
    STREQ   R3, [R0], #4 //Write dword if 32bits(Just make aligned write after header )
    BNE     write_padding_bytes
    MOV     R3, R5
    ADD     R5, R10, #0x440
    ADD     R5, R11, LSL #3
    BX      LR
    write_padding_bytes
    MOV     R9, #32
    loop_write_padding_bytes1
    ROR     R3, R3, #24     //Shift to bit 0, remain bit at bit 31
    STRB    R3, [R0], #1    //Write valid byte
    SXTB    R8, R3
    ADDS    R8, #1
    STRBEQ  R8, [R0], #1    //Write padding in case 0xFF
    SUB     R9, #8
    ANDS    R8, R0, #3      //Check aligned address
    LSL     R8, #3
    ADD     R8, R9
    CMP     R8, #32         //Continue padding if aligned dword is not arrived
    BGE     loop_write_padding_bytes1
    RSB     R8, R9, #32
    LSR     R3, R3, R8     //Clear empty bits to zero
    LSL     R3, R3, R8
    ANDS    R8, R0, #1      //Check additional 0x00
    SUB     R0, R8
    LSL     R8, #3
    ADD     R9, R8          //Remain bits after one dword padding
    LSRNE   R3, R3, #8      //Insert additional 0x00
    LSR     R5, R5, R9     //R3 get remain bits
    ORR     R3, R5
    SUB     R2, R9         //Get number of remain empty bits
    ADD     R5, R10, #0x440
    ADD     R5, R11, LSL #3
    BX      LR
    exit_put_block
    STR     R2, [R12, #0]
    STR     R3, [R12, #4]
    POP     {R4 - R12, LR}
    BX      LR
}

static int dct_zeros = 0;
static int dct_nonzeros = 0;
static int dct_before_zeros = 0;
static int jpeg_encode_one_block(jpeg_buf_t *jpeg_buf, uint8_t *bank_addr, int ch_size, float *fdtbl, int DC, const uint16_t (*HTDC)[2], const uint16_t (*HTAC)[2])
{
    int idx, col_off, bank_off, row_off;
    const uint16_t EOB[2] = { HTAC[0x00][0], HTAC[0x00][1] };
    const uint16_t M16zeroes[2] = { HTAC[0xF0][0], HTAC[0xF0][1] };
#if 1
    int val, val0;
    // first non-zero element in reverse order
    int end0pos = 0;
    int diff;
    long long qcoef;

    //val0 = fast_roundf(((int16_t *)bank_addr)[1]*fdtbl[0]);

    qcoef = ((long long)roundf_c(fdtbl[0] * ((float)0x100000000)));

    val0 = (((int16_t *)bank_addr)[1] * qcoef + (long long)0x80000000) >> 32;


    diff = val0 - DC;

    if (diff == 0)
    {
        jpeg_writeBits(jpeg_buf, HTDC[0]);
    }
    else
    {
        uint16_t bits[2];
        jpeg_calcBits(diff, bits);
        jpeg_writeBits(jpeg_buf, HTDC[bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }

    // Quantize/descale/zigzag the coefficients
    /* Speed up following code by load bank offset and Q coef together(one DWORD)*/
    for (int i = 1; i < 64;)
    {
        int nrzeroes = 0;
        do
        {
            idx = r_jpeg_ZigZag[i++];
            bank_off = ((idx >> 3) & 1) * 0x10000 + ((idx >> 3) / 2) * ch_size;
            row_off  = ((idx & 4) >> 2) * 0x4000;
            col_off  = ((idx & 0x1) << 1) + ((idx & 0x2) ? 0 : 1);

            //printf("idx = %2d val = 0x%8x bank_off=0x%5x row_off=0x%4x col_off=0x%4x\n", idx, ((int16_t *)(bank_addr + bank_off + row_off))[col_off], bank_off, row_off, col_off);
            //val = fast_roundf(((int16_t *)(bank_addr + bank_off + row_off))[col_off]*fdtbl[idx]);
            qcoef = ((long long)roundf_c(fdtbl[idx] * ((float)0x100000000)));

            val = (((int16_t *)(bank_addr + bank_off + row_off))[col_off] * qcoef + (long long)0x80000000) >> 32;
            if (0 == ((int16_t *)(bank_addr + bank_off + row_off))[col_off])
            {
                dct_before_zeros++;
            }
            if (val != 0)
            {
                dct_nonzeros++;
                break;
            }
            dct_zeros++;
            nrzeroes++;
        } while (i < 64);

        if (nrzeroes == 63)
        {
            jpeg_writeBits(jpeg_buf, EOB);
            return val0;
        }

        if (val == 0 && i >= 64)
        {
            break;
        }
        end0pos = i - 1;
        //printf("nrzeroes = %d, val %d\n", nrzeroes, val);

        if (nrzeroes >= 16)
        {
            int lng = nrzeroes >> 4;
            for (int nrmarker = 1; nrmarker <= lng; ++nrmarker)
            {
                jpeg_writeBits(jpeg_buf, M16zeroes);
            }
            nrzeroes &= 15;
        }
        uint16_t bits[2];
        jpeg_calcBits(val, bits);
        jpeg_writeBits(jpeg_buf, HTAC[(nrzeroes << 4) + bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }

    if (end0pos != 63)
    {
        jpeg_writeBits(jpeg_buf, EOB);
    }

    return val0;
#else
    int DUQ[64];

    // first non-zero element in reverse order
    int end0pos = 0;
    // Quantize/descale/zigzag the coefficients
    for (int i = 0; i < 64; ++i)
    {

        idx = i;
        bank_off = ((idx >> 3) & 1) * 0x10000 + ((idx >> 3) / 2) * ch_size;
        row_off  = ((idx & 4) >> 2) * 0x4000;
        col_off  = ((idx & 0x1) << 1) + ((idx & 0x2) ? 0 : 1);
        int16_t val = ((int16_t *)(bank_addr + bank_off + row_off))[col_off];
        DUQ[s_jpeg_ZigZag[i]] = fast_roundf(val * fdtbl[i]);
        if (s_jpeg_ZigZag[i] > end0pos && DUQ[s_jpeg_ZigZag[i]])
        {
            end0pos = s_jpeg_ZigZag[i];
        }
    }

    // Encode DC
    int diff = DUQ[0] - DC;
    if (diff == 0)
    {
        jpeg_writeBits(jpeg_buf, HTDC[0]);
    }
    else
    {
        uint16_t bits[2];
        jpeg_calcBits(diff, bits);
        jpeg_writeBits(jpeg_buf, HTDC[bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }

    // Encode ACs
    if (end0pos == 0)
    {
        jpeg_writeBits(jpeg_buf, EOB);
        return DUQ[0];
    }

    for (int i = 1; i <= end0pos; ++i)
    {
        int startpos = i;
        for (; DUQ[i] == 0 && i <= end0pos ; ++i)
        {
        }
        int nrzeroes = i - startpos;
        if (nrzeroes >= 16)
        {
            int lng = nrzeroes >> 4;
            for (int nrmarker = 1; nrmarker <= lng; ++nrmarker)
            {
                jpeg_writeBits(jpeg_buf, M16zeroes);
            }
            nrzeroes &= 15;
        }
        uint16_t bits[2];
        jpeg_calcBits(DUQ[i], bits);
        jpeg_writeBits(jpeg_buf, HTAC[(nrzeroes << 4) + bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }
    if (end0pos != 63)
    {
        jpeg_writeBits(jpeg_buf, EOB);
    }
    return DUQ[0];
#endif
}

#ifdef ARMC4_OPT

#define YUV_ASM_INT16

#ifdef YUV_ASM_TAB

/*
 *R0: mem_addr   Memory destination address
 *R1: bank_addr  Bank source address
 *R2: width      Image width
 *R3:            yuv table addr (RGB565)
 *R4-R6:         Data from image (4 pixels )
 *R9:            Bank offset
 *R10:           3*Image width
 *R7-R8/R11-R12: Tmp data
 *R14:           Current Bank address
 */
static __asm void *JpegCopyBlockFromImage(uint8_t *mem_addr, uint8_t *bank_addr, int32_t width, const int8_t *yub_tab)
{
    PUSH   {R4 - R12, LR}

    MOV    R9,  #0               //Init bank offset
    ADD    R10, R2, R2           //Init width*3
    ADD    R2,  R10, R2
    ch_loop_cp_from_image
    LDMIA  R0,  {R4 - R6}        //Get 4 pixels of one row from bgr image

#if 0   //Consume 8 instructions each rgb565-table lookup               
    //Get Y0 y R8
    LDRB   R4,  [R0, #0]        //Get 1 pixels(unsigned byte) from bgr image
    LDRB   R5,  [R0, #1]
    LDRB   R6,  [R0, #2]

    ORR    R7,  R6, LSR #3
    LSL    R7,  #6
    ORR    R7,  R5, LSR #2
    ADD    R7,  R4, R7, LSL #5
    LDR    R8,  [R3, R7, LSL #2]   //Read yub table unaligned read now

    //Get Y2 y R7
    LDRB   R4,  [R0, #6]        //Get 1 pixels(unsigned byte) from bgr image
    LDRB   R5,  [R0, #7]
    LDRB   R6,  [R0, #8]

    MOV    R7,  #0
    ORR    R7,  R4, LSR #3
    LSL    R7,  6
    ORR    R7,  R5, LSR #2
    LSL    R7,  5
    ORR    R7,  R6, LSR #3
    ADD    R6,  R7, R7
    ADD    R7,  R6
    LDR    R7,  [R3, R7]   //unaligned read now

    SXTB16 R4, R8, ROR #0  //P0 Y V extend
    SXTB16 R5, R7, ROR #0  //P2 Y V extend
    SXTB   R6, R8, ROR #8  //P0 U extend
    SXTB   R7, R7, ROR #8  //P2 U extend

    PKHBT  R6, R5, R4, LSL #16  //Y0-Y2  R4B-R5B
    ADD    R8, R1, R9
    STR    R6, [R8, #0]
    PKHTB  R6, R4, R5, ASR #16  //V0-V2  R4T-R5T
    STR    R6, [R8, #32]
    PKHBT  R6, R7, R6, LSL #16  //U0-U2  R6B-R7B
    STR    R6, [R8, #16]
#endif
    ADD    R14, R1, R9

    //Consume 6 instructions each rgb565-table lookup
    //Get Y0 y R8
    UBFX   R7,  R4,  #19, #5      //Get r
    ADD    R11, R3,  R7, LSL #13
    UBFX   R7,  R4,  #10, #6      //Get g
    ADD    R11, R11, R7, LSL #7
    UBFX   R7,  R4,  #3, #5       //Get b
    LDR    R8,  [R11, R7, LSL #2] //Read yub table unaligned read now

    //Get Y2 y R7
    UBFX   R7,  R6, #3, #5       //Get r
    ADD    R11, R3, R7, LSL #13
    UBFX   R7,  R5, #26, #6      //Get g
    ADD    R11, R11, R7, LSL #7
    UBFX   R7,  R5, #19, #5      //Get b
    LDR    R7,  [R11, R7, LSL #2] //Read yub table unaligned read now

    SXTB16 R11, R8, ROR #0  //P0 Y V extend
    SXTB16 R12, R7, ROR #0  //P2 Y V extend
    SXTB   R8,  R8, ROR #8  //P0 U extend
    SXTB   R7,  R7, ROR #8  //P2 U extend

    PKHBT  R8,  R7, R8, LSL #16    //U0-U2  R7B-R8B
    STR    R8,  [R14, #16]
    PKHBT  R8,  R12, R11, LSL #16  //Y0-Y2  R11B-R12B
    STR    R8,  [R14, #0]
    PKHTB  R8,  R11, R12, ASR #16  //V0-V2  R11T-R12T
    STR    R8,  [R14, #32]

    //Consume 6 instructions each rgb565-table lookup
    //Get Y1 y R8
    UBFX   R7,  R5,  #11, #5      //Get r
    ADD    R11, R3,  R7, LSL #13
    UBFX   R7,  R5,  #2,  #6      //Get g
    ADD    R11, R11, R7, LSL #7
    UBFX   R7,  R4,  #27, #5      //Get b
    LDR    R8,  [R11, R7, LSL #2] //Read yub table unaligned read now

    //Get Y3 y R7
    UBFX   R7,  R6, #27, #5       //Get r
    ADD    R11, R3, R7, LSL #13
    UBFX   R7,  R6, #18, #6       //Get g
    ADD    R11, R11, R7, LSL #7
    UBFX   R7,  R6, #11, #5       //Get b
    LDR    R7,  [R11, R7, LSL #2] //Read yub table unaligned read now

    SXTB16 R11, R8, ROR #0  //P1 Y V extend
    SXTB16 R12, R7, ROR #0  //P3 Y V extend
    SXTB   R8,  R8, ROR #8  //P1 U extend
    SXTB   R7,  R7, ROR #8  //P3 U extend

    PKHBT  R8,  R7, R8, LSL #16  //U1-U3  R6B-R7B
    STR    R8,  [R14, #20]
    PKHBT  R8,  R12, R11, LSL #16  //Y1-Y3  R4B-R5B
    STR    R8,  [R14, #4]
    PKHTB  R8,  R11, R12, ASR #16  //V1-V3  R4T-R5T
    STR    R8,  [R14, #36]

    ANDS   R4, R9, #8       //Check if already put 4 cols
    ADDEQ  R9, #8       //Go to next 2 cols
    SUBNE  R9, #8       //Go to next 2 rows
    ADDNE  R9, #0x4000
    ADD    R0, R2      //Go to next row of this image(Increase 3*width)
    MOV    R5, # -1
    ANDS   R4, R9, R5, LSR #16 //Get 0xFFFF to check if already put 8 bank rows
    BNE    ch_loop_cp_from_image
    //MOV    R9, #0x10000 //Go to next channel for right half of a block
    SUB    R0, R2, LSL #3//R0 go back to first row of image
    ADD    R0, 12         //Go to right half of this block of this image
    ANDS   R4, R9, #0x20000
    BEQ    ch_loop_cp_from_image

    POP    {R4 - R12, LR}
    BX     LR
}
#endif

#ifdef YUV_ASM_INT16

/*
 *R0: mem_addr   Memory destination address
 *R1: bank_addr  Bank source address
 *R2: width      Image width
 *R3-R5:         rgb data of 2 pixels
 *R11-R12/R7-R8: coef data
 *R9:            Bank offset
 *R6-R10:        tmp data
*R14:            current bank address
 */
static __asm void *JpegCopyBlockFromImage(uint8_t *mem_addr, uint8_t *bank_addr, int32_t width, int32_t *coef)
{
    PUSH   {R4 - R12, LR}

    ADD    R10, R2, R2           //Get 3*width
    ADD    R2,  R10, R2
    MOV    R9,  #0

    //rgb->yuv 0x2646 0x4b23 0xE98 0xEA67 0xD599 0x4000 0x4000 0xCA68 0xF598
    //bgr->yuv 0xE98  0x4b23 0x2646    0x4000 0xD599 0xEA67    0xF598 0xCA68 0x4000
    //YUV conversion coef
    LDR    R11, = 0x4B230E98 //YC1-YC0
                  LDR    R12, = 0xEA672646 //UC2-YC2
                                LDR    R7,  = 0xD5994000 //UC1-UC0
                                        LDR    R8,  = 0xCA68F598 //VC1-VC0
                                                //VC2 0x4000
                                                ch_loop_cp_from_image
                                                //Padding logic ??
#if 0
                                                LDMIA  R0,  {R3 - R5}        //Get 4 pixels of one row from bgr image
                                                //Padding logic ??

                                                //Y conversion
                                                LDR    R11, = 0x0
                                                        LDR    R12, = 0x0
                                                                LDR    R14, = 0x00800080

                                                                        //Get Y0 y at top half of R6
                                                                        SXTB   R6, R3, ROR #0
                                                                        SMULBB R6, R6, R11
                                                                        SXTB   R7, R3, ROR #8
                                                                        SMLABT R6, R7, R11, R6
                                                                        SXTB   R7, R3, ROR #16
                                                                        SMLABT R6, R7, R11, R6

                                                                        //Get Y2 y at top half of R8
                                                                        SXTB   R8, R4, ROR #16
                                                                        SMULBB R8, R8, R11
                                                                        SXTB   R7, R4, ROR #24
                                                                        SMLABT R8, R7, R11, R8
                                                                        SXTB   R7, R5, ROR #0
                                                                        SMLABT R8, R7, R11, R8

                                                                        PKHTB  R6, R6, R8, ASR #16  //Y0-Y2  R6-R8
                                                                        SSUB16 R6, R14              //Y0-128  Y2-128
                                                                        ADD    R8, R1, R9
                                                                        STR    R6, [R8, #0]
#endif
                                                                        ADD    R14, R1, R9

                                                                        //Get P0 y at R3/R4
                                                                        LDRB   R3,  [R0, #0]        //Get 1 pixels(unsigned byte) from bgr image
                                                                        LDRB   R4,  [R0, #1]
                                                                        PKHBT  R3,  R3, R4, LSL #16
                                                                        LDRB   R4,  [R0, #2]

                                                                        //Get P2 y at at R5/R4
                                                                        LDRB   R5,  [R0, #6]        //Get 1 pixels(unsigned byte) from bgr image
                                                                        LDRB   R6,  [R0, #7]
                                                                        PKHBT  R5,  R5, R6, LSL #16
                                                                        LDRB   R6,  [R0, #8]
                                                                        PKHBT  R4,  R4, R6, LSL #16
                                                                        LSL    R3,  #1
                                                                        LSL    R4,  #1
                                                                        LSL    R5,  #1

                                                                        SMUAD  R6,  R3, R11
                                                                        SMLABB R6,  R4, R12, R6

                                                                        SMUAD  R10,  R5, R11
                                                                        SMLATB R10,  R4, R12, R10

                                                                        SUB    R6,  #0x00800000
                                                                        SUB    R10, #0x00800000
                                                                        PKHTB  R10, R6, R10, ASR #16  //Y0-Y2  R5-R6
                                                                        STR    R10, [R14, #0]

                                                                        SMUAD  R6,  R3, R7
                                                                        SMLABT R6,  R4, R12, R6

                                                                        SMUAD  R10, R5, R7
                                                                        SMLATT R10, R4, R12, R10

                                                                        PKHTB  R10, R6, R10, ASR #16  //U0-U2  R5-R6
                                                                        STR    R10, [R14, #16]

                                                                        SMUAD  R6,  R3, R8
                                                                        MOV    R3,  #0x4000             //Get VC2
                                                                        //SMULBB R10, R4, R3
                                                                        SMLABB R6,  R4, R3, R6

                                                                        SMUAD  R10, R5, R8
                                                                        SMLATB R10, R4, R3, R10

                                                                        PKHTB  R10, R6, R10, ASR #16  //V0-V2  R5-R6
                                                                        STR    R10, [R14, #32]

                                                                        //Get P1 y at R3/R4
                                                                        LDRB   R3,  [R0, #3]        //Get 1 pixels(unsigned byte) from bgr image
                                                                        LDRB   R4,  [R0, #4]
                                                                        PKHBT  R3,  R3, R4, LSL #16
                                                                        LDRB   R4,  [R0, #5]

                                                                        //Get P3 y at at R5/R4
                                                                        LDRB   R5,  [R0, #9]        //Get 1 pixels(unsigned byte) from bgr image
                                                                        LDRB   R6,  [R0, #10]
                                                                        PKHBT  R5,  R5, R6, LSL #16
                                                                        LDRB   R6,  [R0, #11]
                                                                        PKHBT  R4,  R4, R6, LSL #16
                                                                        LSL    R3,  #1
                                                                        LSL    R4,  #1
                                                                        LSL    R5,  #1

                                                                        SMUAD  R6,  R3, R11
                                                                        SMLABB R6,  R4, R12, R6

                                                                        SMUAD  R10,  R5, R11
                                                                        SMLATB R10,  R4, R12, R10

                                                                        SUB    R6,  #0x00800000
                                                                        SUB    R10, #0x00800000
                                                                        PKHTB  R10, R6, R10, ASR #16  //Y1-Y3  R5-R6
                                                                        STR    R10, [R14, #4]

                                                                        SMUAD  R6,  R3, R7
                                                                        SMLABT R6,  R4, R12, R6

                                                                        SMUAD  R10, R5, R7
                                                                        SMLATT R10, R4, R12, R10

                                                                        PKHTB  R10, R6, R10, ASR #16  //U1-U3  R5-R6
                                                                        STR    R10, [R14, #20]

                                                                        SMUAD  R6,  R3, R8
                                                                        MOV    R3,  #0x4000          //Get VC2
                                                                        SMLABB R6,  R4, R3, R6

                                                                        SMUAD  R10, R5, R8
                                                                        SMLATB R10, R4, R3, R10

                                                                        PKHTB  R10, R6, R10, ASR #16  //V1-V3  R5-R6
                                                                        STR    R10, [R14, #36]

                                                                        ANDS   R4, R9, #8   //Check if already put 4 cols
                                                                        ADDEQ  R9, #8       //Go to next 2 cols
                                                                        SUBNE  R9, #8       //Go to next 2 rows
                                                                        ADDNE  R9, #0x4000
                                                                        ADD    R0, R2       //Go to next row of this image(Increase 3*width)
                                                                        MOV    R5, # -1
                                                                        ANDS   R4, R9, R5, LSR #16 //Get 0xFFFF to check if already put 8 bank rows
                                                                        BNE    ch_loop_cp_from_image
                                                                        //MOV    R9, #0x10000 //Go to next channel for right half of a block
                                                                        SUB    R0, R2, LSL #3 //R0 go back to first row of image
                                                                        ADD    R0, #12         //Go to right half of this block of this image
                                                                        ANDS   R4, R9, #0x20000
                                                                        BEQ    ch_loop_cp_from_image

                                                                        POP     {R4 - R12, LR}
                                                                        BX      LR
}
#endif

#ifdef YUV_ASM_FLOAT

/*
 *R0: mem_addr   Memory destination address
 *R1: bank_addr  Bank source address
 *R2: width      Image width
 *R3:            yuv coef addr
 *R4-R7:         Data to Bank(4 cols, 2 rows)
 *R8-R9:         Temp Data
 */
static __asm void *JpegCopyBlockFromImage(uint8_t *mem_addr, uint8_t *bank_addr, int32_t width, float *coef)
{
    PUSH   {R4 - R10, LR}

    VLDMIA R3, {S16 - S25}

    ADD    R10, R2, R2             //Get 3*width
    ADD    R10, R2
    MOV    R9,  #0
    ch_loop_cp_from_image
    LDMIA  R0,  {R4 - R6}        //Get 4 pixels of one row from bgr image
    //Padding logic ??

    //Yuv conversion
#if 1    //Int to float
    SBFX  R7, R4, #0,  #8
    VMOV S4, R7
    VCVT.F32.S32 S4,  S4
    SBFX  R7, R4, #8,  #8
    VMOV S5, R7
    VCVT.F32.S32 S5,  S5
    SBFX  R7, R4, #16, #8
    VMOV S6, R7
    VCVT.F32.S32 S6,  S6
    SBFX R7, R4, #24, #8
    VMOV S7, R7
    VCVT.F32.S32 S7,  S7
    SBFX  R7, R5, #0,  #8
    VMOV S8, R7
    VCVT.F32.S32 S8,  S8
    SBFX  R7, R5, #8,  #8
    VMOV S9, R7
    VCVT.F32.S32 S9,  S9
    SBFX  R7, R5, #16, #8
    VMOV S10, R7
    VCVT.F32.S32 S10, S10
    SBFX  R7, R5, #24, #8
    VMOV S11, R7
    VCVT.F32.S32 S11, S11
    SBFX  R7, R6, #0,  #8
    VMOV S12, R7
    VCVT.F32.S32 S12, S12
    SBFX  R7, R6, #8,  #8
    VMOV S13, R7
    VCVT.F32.S32 S13, S13
    SBFX  R7, R6, #16, #8
    VMOV S14, R7
    VCVT.F32.S32 S14, S14
    SBFX  R7, R6, #24, #8
    VMOV S15, R7
    VCVT.F32.S32 S15, S15
#endif

#if 1  //Y conversion of 4 pixels 
    VMUL.F32 S0, S4,  S16
    VMLA.F32 S0, S5,  S17
    VMLA.F32 S0, S6,  S18
    VSUB.F32 S0, S19

    VMUL.F32 S1, S7,  S16
    VMLA.F32 S1, S8,  S17
    VMLA.F32 S1, S9,  S18
    VSUB.F32 S1, S19

    VMUL.F32 S2, S10, S16
    VMLA.F32 S2, S11, S17
    VMLA.F32 S2, S12, S18
    VSUB.F32 S2, S19

    VMUL.F32 S3, S13, S16
    VMLA.F32 S3, S14, S17
    VMLA.F32 S3, S15, S18
    VSUB.F32 S3, S19

    VCVT.S32.F32 S0, S0
    VCVT.S32.F32 S1, S1
    VCVT.S32.F32 S2, S2
    VCVT.S32.F32 S3, S3
    VMOV R4, S0
    VMOV R5, S1
    VMOV R6, S2
    VMOV R7, S3
    ADD    R8, R1, R9           //Get address of 4x4 sub-block
    PKHBT  R4, R6, R4, LSL #16  //Y0-Y2  R4-R6
    PKHBT  R5, R7, R5, LSL #16  //Y1-Y3  R5-R7
    STRD   R4, R5, [R8, #0]     //Store bank rows 0, 1
#endif

#if 1   //U conversion of 4 pixels
    VMUL.F32 S0, S4,  S20
    VMLA.F32 S0, S5,  S21
    VMLA.F32 S0, S6,  S22

    VMUL.F32 S1, S7,  S20
    VMLA.F32 S1, S8,  S21
    VMLA.F32 S1, S9,  S22

    VMUL.F32 S2, S10, S20
    VMLA.F32 S2, S11, S21
    VMLA.F32 S2, S12, S22

    VMUL.F32 S3, S13, S20
    VMLA.F32 S3, S14, S21
    VMLA.F32 S3, S15, S22

    VCVT.S32.F32 S0, S0
    VCVT.S32.F32 S1, S1
    VCVT.S32.F32 S2, S2
    VCVT.S32.F32 S3, S3
    VMOV R4, S0
    VMOV R5, S1
    VMOV R6, S2
    VMOV R7, S3
    PKHBT  R4, R6, R4, LSL #16  //Y0-Y2  R4-R6
    PKHBT  R5, R7, R5, LSL #16  //Y1-Y3  R5-R7
    STRD   R4, R5, [R8, #16]    //Store bank rows 4, 5
#endif

#if 1   //V conversion of 4 pixels
    VMUL.F32 S0, S4,  S23
    VMLA.F32 S0, S5,  S24
    VMLA.F32 S0, S6,  S25

    VMUL.F32 S1, S7,  S23
    VMLA.F32 S1, S8,  S24
    VMLA.F32 S1, S9,  S25

    VMUL.F32 S2, S10, S23
    VMLA.F32 S2, S11, S24
    VMLA.F32 S2, S12, S25

    VMUL.F32 S3, S13, S23
    VMLA.F32 S3, S14, S24
    VMLA.F32 S3, S15, S25

    VCVT.S32.F32 S0, S0
    VCVT.S32.F32 S1, S1
    VCVT.S32.F32 S2, S2
    VCVT.S32.F32 S3, S3
    VMOV R4, S0
    VMOV R5, S1
    VMOV R6, S2
    VMOV R7, S3

    PKHBT  R4, R6, R4, LSL #16  //Y0-Y2  R4-R6
    PKHBT  R5, R7, R5, LSL #16  //Y1-Y3  R5-R7
    STRD   R4, R5, [R8, #32]    //Store bank rows 8, 9
#endif
    ANDS   R4, R9, #8       //Check if already put 4 cols
    ADDEQ  R9, #8       //Go to next 2 cols
    SUBNE  R9, #8       //Go to next 2 rows
    ADDNE  R9, #0x4000
    ADD    R0, R10      //Go to next row of this image(Increase 3*width)
    CMP    R9, #0x10000 //Check if already put 8 bank rows
    BNE    ch_loop_cp_from_image
    //MOV    R9, #0x10000 //Go to next channel for right half of a block
    SUB    R0, R10, LSL #3//R0 go back to first row of image
    ADD    R0, 12         //Go to right half of this block of this image
    ANDS   R4, R9, #0x20000
    BEQ    ch_loop_cp_from_image

    POP     {R4 - R10, LR}
    BX      lr
}
#endif
#endif

static void image_get_blocks(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, bank_col, bank_row, bank_off, off_row, off_col;
    int32_t start_row, start_col, row_blocks;
    uint8_t *bank_addr  = bank_start_addr;
    uint8_t *block_addr;
    int i, j;
    float r, g, b;
    int16_t yuv[12]; /* Convert from RGB888 to YUV */
    int32_t pixel_size = 3; /* Fixed to RGB888 now */

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;
    block_addr = &image_addr[(start_row * width + start_col) * pixel_size];

    bank_col = 0;
    bank_row = 0;
    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
#ifdef ARMC4_OPT

#ifdef YUV_ASM_TAB
        extern int8_t yuv_table[];
        /* It is the best that image and yuv_table in cached memory */
        JpegCopyBlockFromImage(block_addr, bank_addr + bank_col * 4, width, yuv_table);
#endif
#ifdef YUV_ASM_INT16
        int32_t coef[9] = {(int32_t)(0.29900f * 65536.0f), (int32_t)(0.58700f * 65536.0f), (int32_t)(0.11400f * 65536.0f), (int32_t)(-0.16874f * 65536.0f), (int32_t)(-0.33126f * 65536.0f), (int32_t)(0.50000f * 65536.0f), (int32_t)(0.50000f * 65536.0f), (int32_t)(-0.41869f * 65536.0f), (int32_t)(-0.08131f * 65536.0f)};
        /*
        coef[0] = roundf_c(0.29900f *32768.0f);
        coef[1] = roundf_c(0.58700f *32768.0f);
        coef[2] = roundf_c(0.11400f *32768.0f);
        coef[3] = roundf_c(-0.16874f*32768.0f);
        coef[4] = roundf_c(-0.33126f*32768.0f);
        coef[5] = roundf_c(0.50000f *32768.0f);
        coef[6] = roundf_c(0.50000f *32768.0f);
        coef[7] = roundf_c(-0.41869f*32768.0f);
        coef[8] = roundf_c(-0.08131f*32768.0f);
        //0x2646 0x4b23 0xE98 0xEA67 0xD599 0x4000 0x4000 0xCA68 0xF598
        */
        JpegCopyBlockFromImage(block_addr, bank_addr + bank_col * 4, width, coef);
#endif
#ifdef YUV_ASM_FLOAT
        float coef[10] = {0.29900f, 0.58700f, 0.11400f, 128.0f, -0.16874f, -0.33126f, 0.50000f, 0.50000f, -0.41869f, -0.08131f};
        JpegCopyBlockFromImage(block_addr, bank_addr + bank_col * 4, width, coef);
#endif
#else
        /* Get one sub-block as one input channel */
        /* A00 A01 A02 A03          A00 A01 A10 A11
         * A10 A11 A12 A13   -----> A02 A03 A12 A13  -----> A02 A00 A03 A01 A12 A10 A13 A11
         * A20 A21 A22 A23   -----> A20 A21 A30 A31  -----> A22 A20 A23 A21 A32 A30 A33 A31
         * A30 A31 A32 A33          A22 A23 A32 A33
         */
        /* Covert 4 pixels of each line of rgb image into yuv */
        for (off_col = 0; off_col < 8; off_col += 4)
        {
            /* Process 4x4 sub block */
            for (off_row = 0; off_row < 8; off_row++)
            {
                for (i = 0, j = 0; i < 12; i += 3, j++)
                {
                    b = block_addr[3 * off_row * width + 3 * off_col + i + 0];
                    g = block_addr[3 * off_row * width + 3 * off_col + i + 1];
                    r = block_addr[3 * off_row * width + 3 * off_col + i + 2];

                    yuv[j + 0] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                    yuv[j + 4] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                    yuv[j + 8] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
                }


                /* Padding rows and cols */
                if ((col + off_col + 3) >= width || (row + off_row) >= height)
                {
                    if ((row + off_row) >= height)
                    {
                        for (j = 0; j < 4; j++)
                        {
                            yuv[j + 0] =  yuv[j + 4] =  yuv[j + 8] = 0;
                        }
                    }
                    else
                    {
                        for (j = 3; (col + off_col + j) >= width; j--)
                        {
                            yuv[j + 0] =  yuv[j + 4] =  yuv[j + 8] = 0;
                        }
                    }
                }


                /* Add 4 rows for bottom 4 rows, add 1 col for odd row, add 2 rows for bottom 2 rows */
                bank_off =  bank_col + ((off_row & 0x1) << 1) + ((off_row & 0x6) << 11);
                ((uint32_t *)bank_addr)[bank_off + 0] = (uint16_t)yuv[2] + (((uint16_t)yuv[0]) << 16); /* Y */
                ((uint32_t *)bank_addr)[bank_off + 1] = (uint16_t)yuv[3] + (((uint16_t)yuv[1]) << 16);
                ((uint32_t *)bank_addr)[bank_off + 4] = (uint16_t)yuv[6] + (((uint16_t)yuv[4]) << 16); /* U */
                ((uint32_t *)bank_addr)[bank_off + 5] = (uint16_t)yuv[7] + (((uint16_t)yuv[5]) << 16);
                ((uint32_t *)bank_addr)[bank_off + 8] = (uint16_t)yuv[10] + (((uint16_t)yuv[8]) << 16); /* V */
                ((uint32_t *)bank_addr)[bank_off + 9] = (uint16_t)yuv[11] + (((uint16_t)yuv[9]) << 16);
            }
            bank_addr += 0x10000; /* Odd channel for bottom half cols */
        }
        bank_addr -= 0x20000;
#endif

        bank_col  += 12;
        /* Change bank address if 4 bank rows fullfilled */
        if (bank_col >= len_col)
        {
            bank_col   = 0;
            bank_row  += 8;
            bank_addr += 4 * len_col;
        }
        /* Get next macro block from image */
        col += 8;
        block_addr += 8 * pixel_size;
        if (col >= width)
        {
            col = 0;
            row += 8;
            block_addr = image_addr + row * width * pixel_size;
        }
    }
}

static void check_get_blocks(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, bank_col, bank_row, bank_off, off_row, off_col;
    int32_t start_row, start_col, row_blocks;
    uint8_t *bank_addr  = bank_start_addr;
    uint8_t *block_addr;
    int i, j;
    float r, g, b;
    int16_t yuv[12]; /* Convert from RGB888 to YUV */
    int32_t pixel_size = 3; /* Fixed to RGB888 now */
    uint32_t bank_val;

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;
    block_addr = &image_addr[(start_row * width + start_col) * pixel_size];

    bank_col = 0;
    bank_row = 0;
    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
        /* Get one sub-block as one input channel */
        /* A00 A01 A02 A03          A00 A01 A10 A11
         * A10 A11 A12 A13   -----> A02 A03 A12 A13  -----> A02 A00 A03 A01 A12 A10 A13 A11
         * A20 A21 A22 A23   -----> A20 A21 A30 A31  -----> A22 A20 A23 A21 A32 A30 A33 A31
         * A30 A31 A32 A33          A22 A23 A32 A33
         */
        /* Covert 4 pixels of each line of rgb image into yuv */
        for (off_col = 0; off_col < 8; off_col += 4)
        {
            /* Process 4x4 sub block */
            for (off_row = 0; off_row < 8; off_row++)
            {
                for (i = 0, j = 0; i < 12; i += 3, j++)
                {
                    b = block_addr[3 * off_row * width + 3 * off_col + i + 0];
                    g = block_addr[3 * off_row * width + 3 * off_col + i + 1];
                    r = block_addr[3 * off_row * width + 3 * off_col + i + 2];

                    yuv[j + 0] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                    yuv[j + 4] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                    yuv[j + 8] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
                }


                /* Padding rows and cols */
                if ((col + off_col + 3) >= width || (row + off_row) >= height)
                {
                    if ((row + off_row) >= height)
                    {
                        for (j = 0; j < 4; j++)
                        {
                            yuv[j + 0] =  yuv[j + 4] =  yuv[j + 8] = 0;
                        }
                    }
                    else
                    {
                        for (j = 3; (col + off_col + j) >= width; j--)
                        {
                            yuv[j + 0] =  yuv[j + 4] =  yuv[j + 8] = 0;
                        }
                    }
                }


                /* Add 4 rows for bottom 4 rows, add 1 col for odd row, add 2 rows for bottom 2 rows */
                bank_off =  bank_col + ((off_row & 0x1) << 1) + ((off_row & 0x6) << 11);
                bank_val = ((uint32_t *)bank_addr)[bank_off + 0];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[0]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[2]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }

                bank_val = ((uint32_t *)bank_addr)[bank_off + 1];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[1]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[3]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }
                bank_val = ((uint32_t *)bank_addr)[bank_off + 4];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[4]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[6]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }

                bank_val = ((uint32_t *)bank_addr)[bank_off + 5];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[5]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[7]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }
                bank_val = ((uint32_t *)bank_addr)[bank_off + 8];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[8]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[10]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }

                bank_val = ((uint32_t *)bank_addr)[bank_off + 9];
                if (abs(((int16_t)(bank_val >> 16)) - yuv[9]) > 2 || abs(((int16_t)(bank_val & 0xFFFF)) - yuv[11]) > 2)
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }
#if 0
                if (((uint32_t *)bank_addr)[bank_off + 0] != (uint32_t)yuv[2] + (((uint32_t)yuv[0]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }
                if (((uint32_t *)bank_addr)[bank_off + 1] != (uint32_t)yuv[3] + (((uint32_t)yuv[1]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* Y */
                }

                if (((uint32_t *)bank_addr)[bank_off + 4] != (uint32_t)yuv[6] + (((uint32_t)yuv[4]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* U */
                }

                if (((uint32_t *)bank_addr)[bank_off + 5] != (uint32_t)yuv[7] + (((uint32_t)yuv[5]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* U */
                }

                if (((uint32_t *)bank_addr)[bank_off + 8] != (uint32_t)yuv[10] + (((uint32_t)yuv[8]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* V */
                }

                if (((uint32_t *)bank_addr)[bank_off + 9] != (uint32_t)yuv[11] + (((uint32_t)yuv[9]) << 16))
                {
                    report_error(ERROR_GET_BLOCK); /* V */
                }
#endif
            }
            bank_addr += 0x10000; /* Odd channel for bottom half cols */
        }
        bank_addr -= 0x20000;

        bank_col  += 12;
        /* Change bank address if 4 bank rows fullfilled */
        if (bank_col >= len_col)
        {
            bank_col   = 0;
            bank_row  += 8;
            bank_addr += 4 * len_col;
        }
        /* Get next macro block from image */
        col += 8;
        block_addr += 8 * pixel_size;
        if (col >= width)
        {
            col = 0;
            row += 8;
            block_addr = image_addr + row * width * pixel_size;
        }
    }
}

/* Merge 8 output channels of row dct into 2 channels */
static void dct_bank_channel_merge(uint8_t *bank_start_addr, uint8_t *dst_bank_start_addr, int32_t len_row, int32_t len_col)
{
    int32_t row, col, out_ch, ch_size;
    uint8_t *bank_addr, *dst_bank_addr;

    /*
     *   AT0-AB0/AT1-AB1         \
     *   AT2-AB2/AT3-AB3    ------\
     *   AT4-AB4/AT5-AB5    ------/
     *   AT6-AB6/AT7-AB7         /
     *              ||
     *             \||/
     *              \/
     *   AT0-AT1-AT2-AT3/AB0-AB1-AB2-AB3
     *   AT4-AT5-AT6-AT7/AB4-AB5-AB6-AB7
     */
    ch_size = ((len_row + 7) / 8) * len_col;

    for (out_ch = 0; out_ch < 2; out_ch++)
    {
        /* Get top 4 cols at row 0, get bottom 4 cols at row 2 */
        bank_addr = bank_start_addr + out_ch * 0x4000;
        /* Put top 4 cols at even channel, put bottom 4 cols at odd channel */
        dst_bank_addr  = dst_bank_start_addr + out_ch * 0x10000;

        for (row = 0; row < len_row;)
        {
            for (col = 0; col < len_col; col += 2)
            {
                ((uint32_t *)dst_bank_addr)[2 * col + 0] = ((uint32_t *)bank_addr)[col + 0 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 1] = ((uint32_t *)bank_addr)[col + 0 * ch_size + 1];

                ((uint32_t *)dst_bank_addr)[2 * col + 2] = ((uint32_t *)(bank_addr + 0x10000))[col + 0 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 3] = ((uint32_t *)(bank_addr + 0x10000))[col + 0 * ch_size + 1];

                dst_bank_addr += 0x4000;

                ((uint32_t *)dst_bank_addr)[2 * col + 0] = ((uint32_t *)bank_addr)[col + 1 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 1] = ((uint32_t *)bank_addr)[col + 1 * ch_size + 1];

                ((uint32_t *)dst_bank_addr)[2 * col + 2] = ((uint32_t *)(bank_addr + 0x10000))[col + 1 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 3] = ((uint32_t *)(bank_addr + 0x10000))[col + 1 * ch_size + 1];

                dst_bank_addr += 0x4000;

                ((uint32_t *)dst_bank_addr)[2 * col + 0] = ((uint32_t *)bank_addr)[col + 2 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 1] = ((uint32_t *)bank_addr)[col + 2 * ch_size + 1];

                ((uint32_t *)dst_bank_addr)[2 * col + 2] = ((uint32_t *)(bank_addr + 0x10000))[col + 2 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 3] = ((uint32_t *)(bank_addr + 0x10000))[col + 2 * ch_size + 1];

                dst_bank_addr += 0x4000;

                ((uint32_t *)dst_bank_addr)[2 * col + 0] = ((uint32_t *)bank_addr)[col + 3 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 1] = ((uint32_t *)bank_addr)[col + 3 * ch_size + 1];

                ((uint32_t *)dst_bank_addr)[2 * col + 2] = ((uint32_t *)(bank_addr + 0x10000))[col + 3 * ch_size];
                ((uint32_t *)dst_bank_addr)[2 * col + 3] = ((uint32_t *)(bank_addr + 0x10000))[col + 3 * ch_size + 1];

                dst_bank_addr -= 0xc000;
            }

            dst_bank_addr += 2 * 4 * len_col;

            row += 4;
            /* Inrease 8 rows to pointer to bank */
            if ((row & 0x7) == 0)
            {
                bank_addr += 0x8000;
                bank_addr -= 0x10000;
                bank_addr += 4 * len_col;
            }
            else
            {
                bank_addr += 0x8000;
            }
        }
    }
}

static void dct_butterfly(int *DU, int *CDU, int row_col_sel)
{
    //int DUQ[64];
    int z1, z2, z3, z4, z5, z11, z13;
    int t0, t1, t2, t3, t4, t5, t6, t7, t10, t11, t12, t13;

    // DCT rows
    if (row_col_sel == 0)
    {
        for (int i = 8, *p = DU; i > 0; i--, p += 8, CDU += 8)
        {
            t0 = CDU[0] + CDU[7];
            t1 = CDU[1] + CDU[6];
            t2 = CDU[2] + CDU[5];
            t3 = CDU[3] + CDU[4];

            t7 = CDU[0] - CDU[7];
            t6 = CDU[1] - CDU[6];
            t5 = CDU[2] - CDU[5];
            t4 = CDU[3] - CDU[4];

            // Even part
            t10 = t0 + t3;
            t13 = t0 - t3;
            t11 = t1 + t2;
            t12 = t1 - t2;
            z1 = MULTIPLY(t12 + t13, FIX_0_707106781); // c4

            p[0] = t10 + t11;
            p[4] = t10 - t11;
            p[2] = DESCALE(t13 * FIX_1_0000000 + z1, 8);
            p[6] = DESCALE(t13 * FIX_1_0000000 - z1, 8);

            // Odd part
            t10 = t4 + t5;// phase 2
            t11 = t5 + t6;
            t12 = t6 + t7;

            // The rotator is modified from fig 4-8 to avoid extra negations.
            z5 = MULTIPLY(t10 - t12, FIX_0_382683433); // c6
            z2 = MULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
            z4 = MULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
            z3 = MULTIPLY(t11, FIX_0_707106781); // c4
            z11 = t7 * FIX_1_0000000 + z3;  // phase 5
            z13 = t7 * FIX_1_0000000 - z3;

            p[5] = DESCALE(z13 + z2, 8);// phase 6
            p[3] = DESCALE(z13 - z2, 8);
            p[1] = DESCALE(z11 + z4, 8);
            p[7] = DESCALE(z11 - z4, 8);
        }
    }
    else
    {
        // DCT columns
        for (int i = 8, *p = DU; i > 0; i--, p++)
        {
            t0 = p[0]  + p[56];
            t1 = p[8]  + p[48];
            t2 = p[16] + p[40];
            t3 = p[24] + p[32];

            t7 = p[0]  - p[56];
            t6 = p[8]  - p[48];
            t5 = p[16] - p[40];
            t4 = p[24] - p[32];

            // Even part
            t10 = t0 + t3;  // phase 2
            t13 = t0 - t3;
            t11 = t1 + t2;
            t12 = t1 - t2;
            z1 = MULTIPLY(t12 + t13, FIX_0_707106781); // c4

            p[0]  = t10 + t11;      // phase 3
            p[32] = t10 - t11;
            p[16] = DESCALE(t13 * FIX_1_0000000 + z1, 8);       // phase 5
            p[48] = DESCALE(t13 * FIX_1_0000000 - z1, 8);

            // Odd part
            t10 = t4 + t5;      // phase 2
            t11 = t5 + t6;
            t12 = t6 + t7;

            // The rotator is modified from fig 4-8 to avoid extra negations.
            z5 = MULTIPLY(t10 - t12, FIX_0_382683433); // c6
            z2 = MULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
            z4 = MULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
            z3 = MULTIPLY(t11, FIX_0_707106781); // c4
            z11 = t7 * FIX_1_0000000 + z3;      // phase 5
            z13 = t7 * FIX_1_0000000 - z3;

            p[40] = DESCALE(z13 + z2, 8);// phase 6
            p[24] = DESCALE(z13 - z2, 8);
            p[8]  = DESCALE(z11 + z4, 8);
            p[56] = DESCALE(z11 - z4, 8);
        }
    }
}

static void check_row_dct(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, off_row, off_col, ch_size, bank_off, bank_col;
    int32_t start_row, start_col, row_blocks, col_blocks;
    uint8_t *bank_addr;
    uint8_t *block_addr;
    int i, out_ch;
    float r, g, b;
    int32_t pixel_size = 3; /* Fixed to RGB888 now */

    int ydu_idx, udu_idx, vdu_idx;
    int YDU[64], UDU[64], VDU[64];
    int CYDU[64], CUDU[64], CVDU[64];
    int BYDU[64], BUDU[64], BVDU[64];
    int *PDU;

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;

    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
        block_addr = &image_addr[(row * width + col) * pixel_size];

        ydu_idx = udu_idx = vdu_idx = 0;
        for (off_row = 0; off_row < 8; off_row++)
        {
            for (off_col = 0; off_col < 8; off_col++)
            {
                b = block_addr[3 * off_row * width + 3 * off_col + 0];
                g = block_addr[3 * off_row * width + 3 * off_col + 1];
                r = block_addr[3 * off_row * width + 3 * off_col + 2];

                YDU[ydu_idx++] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                UDU[udu_idx++] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                VDU[vdu_idx++] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
            }
        }

        dct_butterfly(CYDU, YDU, 0);
        dct_butterfly(CUDU, UDU, 0);
        dct_butterfly(CVDU, VDU, 0);

        ch_size = ((len_row + 7) / 8) * len_col * 4;
        /* Each block is 4 rows and 6 cols (24bytes)*/
        col_blocks = len_col / 6;
        row_blocks = len_col / 3; /* blocks for each 8 rows */
        bank_addr  = bank_start_addr + (block / row_blocks) * len_col * 4 + ((block % row_blocks) / col_blocks) * 0x8000 + ((block % row_blocks) % col_blocks) * 6 * 4;

        for (out_ch = 0; out_ch < 8; out_ch++)
        {
            /* Get top 4 cols at row 0, get bottom 4 cols at row 2 */
            bank_off = (out_ch & 1) * 0x10000 + (out_ch / 2) * ch_size;
            {
                for (bank_col = 0; bank_col < 6; bank_col += 2)
                {
                    PDU = BYDU;
                    if ((bank_col % 6) == 2)
                    {
                        PDU = BUDU;
                    }
                    else if ((bank_col % 6) == 4)
                    {
                        PDU = BVDU;
                    }
                    PDU[out_ch + 0]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 1];
                    PDU[out_ch + 8]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 3];
                    PDU[out_ch + 16] = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 0];
                    PDU[out_ch + 24] = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 2];
                    PDU[out_ch + 32] = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 1];
                    PDU[out_ch + 40] = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 3];
                    PDU[out_ch + 48] = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 0];
                    PDU[out_ch + 56] = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 2];

                }
            }
        }

        for (i = 0; i < 64; i++)
        {
            if (BYDU[i] != CYDU[i])
            {
                printf("dct Y error block = %d %d kdp=%d cpu=%d\n", block, i, BYDU[i], CYDU[i]);
            }
            if (BUDU[i] != CUDU[i])
            {
                printf("dct U error block = %d %d kdp=%d cpu=%d\n", block, i, BUDU[i], CUDU[i]);
            }
            if (BVDU[i] != CVDU[i])
            {
                printf("dct V error block = %d %d kdp=%d cpu=%d\n", block, i, BVDU[i], CVDU[i]);
            }
        }


        col += 8;
        if (col >= width)
        {
            col = 0;
            row += 8;
        }
    }
}

static void copy_row_dct(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, off_row, off_col, ch_size, bank_off, bank_col;
    int32_t start_row, start_col, row_blocks, col_blocks;
    uint8_t *bank_addr;
    uint8_t *block_addr;
    int out_ch;
    float r, g, b;
    int32_t pixel_size = 3; /* Fixed to RGB888 now */

    int ydu_idx, udu_idx, vdu_idx;
    int YDU[64], UDU[64], VDU[64];
    int CYDU[64], CUDU[64], CVDU[64];
    int *PDU;

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;

    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
        block_addr = &image_addr[(row * width + col) * pixel_size];

        ydu_idx = udu_idx = vdu_idx = 0;
        for (off_row = 0; off_row < 8; off_row++)
        {
            for (off_col = 0; off_col < 8; off_col++)
            {
                b = block_addr[3 * off_row * width + 3 * off_col + 0];
                g = block_addr[3 * off_row * width + 3 * off_col + 1];
                r = block_addr[3 * off_row * width + 3 * off_col + 2];

                YDU[ydu_idx++] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                UDU[udu_idx++] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                VDU[vdu_idx++] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
            }
        }

        dct_butterfly(CYDU, YDU, 0);
        dct_butterfly(CUDU, UDU, 0);
        dct_butterfly(CVDU, VDU, 0);

        ch_size = ((len_row + 7) / 8) * len_col * 4;
        /* Each block is 4 rows and 6 cols (24bytes)*/
        col_blocks = len_col / 6;
        row_blocks = len_col / 3; /* blocks for each 8 rows */
        bank_addr  = bank_start_addr + (block / row_blocks) * len_col * 4 + ((block % row_blocks) / col_blocks) * 0x8000 + ((block % row_blocks) % col_blocks) * 6 * 4;

        for (out_ch = 0; out_ch < 8; out_ch++)
        {
            /* Get top 4 cols at row 0, get bottom 4 cols at row 2 */
            bank_off = (out_ch & 1) * 0x10000 + (out_ch / 2) * ch_size;
            {
                for (bank_col = 0; bank_col < 6; bank_col += 2)
                {
                    PDU = CYDU;
                    if ((bank_col % 6) == 2)
                    {
                        PDU = CUDU;
                    }
                    else if ((bank_col % 6) == 4)
                    {
                        PDU = CVDU;
                    }
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 1]         = PDU[out_ch + 0];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 3]         = PDU[out_ch + 8];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 0]         = PDU[out_ch + 16];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 2]         = PDU[out_ch + 24];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 1]  = PDU[out_ch + 32];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 3]  = PDU[out_ch + 40];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 0]  = PDU[out_ch + 48];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 2]  = PDU[out_ch + 56];
                }
            }
        }
        col += 8;
        if (col >= width)
        {
            col = 0;
            row += 8;
        }
    }
}

static void check_col_dct(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, off_row, off_col, ch_size, bank_off, bank_col;
    int32_t start_row, start_col, row_blocks, col_blocks;
    uint8_t *bank_addr;
    uint8_t *block_addr;
    int i, out_ch;
    float r, g, b;
    int32_t pixel_size = 3; /* Fixed to RGB888 now */

    int ydu_idx, udu_idx, vdu_idx;
    int YDU[64], UDU[64], VDU[64];
    int CYDU[64], CUDU[64], CVDU[64];
    int BYDU[64], BUDU[64], BVDU[64];
    int *PDU;

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;

    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
        block_addr = &image_addr[(row * width + col) * pixel_size];

        ydu_idx = udu_idx = vdu_idx = 0;
        for (off_row = 0; off_row < 8; off_row++)
        {
            for (off_col = 0; off_col < 8; off_col++)
            {
                b = block_addr[3 * off_row * width + 3 * off_col + 0];
                g = block_addr[3 * off_row * width + 3 * off_col + 1];
                r = block_addr[3 * off_row * width + 3 * off_col + 2];

                YDU[ydu_idx++] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                UDU[udu_idx++] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                VDU[vdu_idx++] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
            }
        }

        dct_butterfly(CYDU, YDU, 0);
        dct_butterfly(CYDU, YDU, 1);
        dct_butterfly(CUDU, UDU, 0);
        dct_butterfly(CUDU, UDU, 1);
        dct_butterfly(CVDU, VDU, 0);
        dct_butterfly(CVDU, VDU, 1);

        ch_size = ((len_row + 7) / 8) * len_col * 4;
        /* Each block is 2 rows and 12 cols (48bytes)*/
        col_blocks = len_col / 6;
        row_blocks = len_col / 3; /* blocks for each 8 rows */
        bank_addr  = bank_start_addr + (block / row_blocks) * len_col * 4 + ((block % row_blocks) / col_blocks) * 0x8000 + ((block % row_blocks) % col_blocks) * 6 * 4;

        for (out_ch = 0; out_ch < 8; out_ch++)
        {
            /* Get top 4 cols at row 0, get bottom 4 cols at row 2 */
            bank_off = (out_ch & 1) * 0x10000 + (out_ch / 2) * ch_size;
            {
                for (bank_col = 0; bank_col < 6; bank_col += 2)
                {
                    PDU = BYDU;
                    if (bank_col >= 4)
                    {
                        PDU = BVDU;
                    }
                    else if (bank_col >= 2)
                    {
                        PDU = BUDU;
                    }
                    PDU[out_ch * 8 + 0]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 1];
                    PDU[out_ch * 8 + 1]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 3];
                    PDU[out_ch * 8 + 2]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 0];
                    PDU[out_ch * 8 + 3]  = ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 2];
                    PDU[out_ch * 8 + 4]  = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 1];
                    PDU[out_ch * 8 + 5]  = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 3];
                    PDU[out_ch * 8 + 6]  = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 0];
                    PDU[out_ch * 8 + 7]  = ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 2];

                }
            }
        }

        for (i = 0; i < 64; i++)
        {
            if (BYDU[i] != CYDU[i])
            {
                printf("dct Y error block = %d %d kdp=%d cpu=%d\n", block, i, BYDU[i], CYDU[i]);
            }
            if (BUDU[i] != CUDU[i])
            {
                printf("dct U error block = %d %d kdp=%d cpu=%d\n", block, i, BUDU[i], CUDU[i]);
            }
            if (BVDU[i] != CVDU[i])
            {
                printf("dct V error block = %d %d kdp=%d cpu=%d\n", block, i, BVDU[i], CVDU[i]);
            }
        }

        if (block == 0)
        {
            for (i = 0; i < 64; ++i)
            {
                CYDU[s_jpeg_ZigZag[i]] = BYDU[i];
            }

            for (i = 0; i < 64; ++i)
            {
                printf("idx = %2d val = 0x%8x\n", r_jpeg_ZigZag[i], CYDU[i]);
            }
        }
        col += 8;
        if (col >= width)
        {
            col = 0;
            row += 8;
        }
    }
}

static void copy_col_dct(uint8_t *image_addr, uint8_t *bank_start_addr, int32_t bpp, int32_t start_block, int32_t len_block, int32_t width, int32_t height, int32_t ch_num, int32_t len_row, int32_t len_col)
{
    int32_t row, col, block, off_row, off_col, ch_size, bank_off, bank_col;
    int32_t start_row, start_col, row_blocks, col_blocks;
    uint8_t *bank_addr;
    uint8_t *block_addr;
    int out_ch;
    float r, g, b;
    int32_t pixel_size = 3; /* Fixed to RGB888 now */

    int ydu_idx, udu_idx, vdu_idx;
    int YDU[64], UDU[64], VDU[64];
    int CYDU[64], CUDU[64], CVDU[64];
    int *PDU;

    row_blocks = (width + 7) / 8;
    start_row  = (start_block / row_blocks) * 8;
    start_col  = (start_block % row_blocks) * 8;

    for (block = 0, row = start_row, col = start_col; (block < len_block || block < 8); ++block)
    {
        block_addr = &image_addr[(row * width + col) * pixel_size];

        ydu_idx = udu_idx = vdu_idx = 0;
        for (off_row = 0; off_row < 8; off_row++)
        {
            for (off_col = 0; off_col < 8; off_col++)
            {
                b = block_addr[3 * off_row * width + 3 * off_col + 0];
                g = block_addr[3 * off_row * width + 3 * off_col + 1];
                r = block_addr[3 * off_row * width + 3 * off_col + 2];

                YDU[ydu_idx++] = +0.29900f * r + 0.58700f * g + 0.11400f * b - 128;
                UDU[udu_idx++] = -0.16874f * r - 0.33126f * g + 0.50000f * b;
                VDU[vdu_idx++] = +0.50000f * r - 0.41869f * g - 0.08131f * b;
            }
        }

        dct_butterfly(CYDU, YDU, 0);
        dct_butterfly(CYDU, YDU, 1);
        dct_butterfly(CUDU, UDU, 0);
        dct_butterfly(CUDU, UDU, 1);
        dct_butterfly(CVDU, VDU, 0);
        dct_butterfly(CVDU, VDU, 1);

        ch_size = ((len_row + 7) / 8) * len_col * 4;
        /* Each block is 2 rows and 12 cols (48bytes)*/
        col_blocks = len_col / 6;
        row_blocks = len_col / 3; /* blocks for each 8 rows */
        bank_addr  = bank_start_addr + (block / row_blocks) * len_col * 4 + ((block % row_blocks) / col_blocks) * 0x8000 + ((block % row_blocks) % col_blocks) * 6 * 4;

        for (out_ch = 0; out_ch < 8; out_ch++)
        {
            /* Get top 4 cols at row 0, get bottom 4 cols at row 2 */
            bank_off = (out_ch & 1) * 0x10000 + (out_ch / 2) * ch_size;
            {
                for (bank_col = 0; bank_col < 6; bank_col += 2)
                {
                    PDU = CYDU;
                    if (bank_col >= 4)
                    {
                        PDU = CVDU;
                    }
                    else if (bank_col >= 2)
                    {
                        PDU = CUDU;
                    }
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 1]          = PDU[out_ch * 8 + 0];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 3]          = PDU[out_ch * 8 + 1];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 0]          = PDU[out_ch * 8 + 2];
                    ((int16_t *)(bank_addr + bank_off))[2 * bank_col + 2]          = PDU[out_ch * 8 + 3];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 1]   = PDU[out_ch * 8 + 4];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 3]   = PDU[out_ch * 8 + 5];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 0]   = PDU[out_ch * 8 + 6];
                    ((int16_t *)(bank_addr + bank_off + 0x4000))[2 * bank_col + 2]   = PDU[out_ch * 8 + 7];

                }
            }
        }

        col += 8;
        if (col >= width)
        {
            col = 0;
            row += 8;
        }
    }
}
uint8_t jpeg_out_check_buf[64 * 1024];

static void jpeg_put_blocks(jpeg_buf_t *jpeg_buf, uint8_t *bank_start_addr, int32_t len_block, int32_t ch_num, int32_t row_num, int32_t col_num)
{
    int32_t row, col, ch_size, block = 0;
    uint8_t *bank_addr = bank_start_addr;

    ch_size = col_num * ((row_num + 7) / 8) * 4;

#if 1
    uint8_t *cur_buf_addr;
    uint32_t remain_bytes, remain_bits;
    int i;

    //Remain bits aligned with dword
    if ((jpeg_buf->idx & 0x3))
    {
        remain_bytes = (jpeg_buf->idx & 0x3);
        cur_buf_addr = jpeg_buf->buf + jpeg_buf->idx - remain_bytes;
        remain_bits = 0;
        for (i = 0; i < remain_bytes; i++)
        {
            remain_bits |= ((cur_buf_addr[i]) << (24 - 8 * i));
        }
        remain_bits |= ((((uint32_t)jpeg_buf->bitb) & 0xFF0000) >> 16) << (24 - 8 * remain_bytes);
    }
    else
    {
        remain_bytes = 0;
        remain_bits = ((((uint32_t)jpeg_buf->bitb) & 0xFF0000) >> 16) << 24;
    }
    jpeg_asm_encode_context.bitc = 32 - (jpeg_buf->bitc + remain_bytes * 8); /* bitc is empty bits */
    jpeg_asm_encode_context.bitb = remain_bits ;
    jpeg_asm_encode_context.row  = 0;
    //jpeg_asm_encode_context.idx = jpeg_buf->idx - remain_bytes;
    jpeg_asm_encode_context.start_block = 0;
    jpeg_asm_encode_context.end_block = len_block;
    jpeg_asm_encode_context.block_idx = 0;
    jpeg_asm_encode_context.table_idx = 0;
    jpeg_asm_encode_context.table_len = 3;
    jpeg_asm_encode_context.col_num = col_num;
    jpeg_asm_encode_context.col = 0;
    jpeg_asm_encode_context.ch_size = ch_size;
    jpeg_asm_encode_context.mt[0] = &Y_MT[0][0];
    jpeg_asm_encode_context.mt[1] = &UV_MT[0][0];
    jpeg_asm_encode_context.mt[2] = &UV_MT[0][0];
    jpeg_asm_encode_context.constant = 0x01010101;

    cur_buf_addr = AsmPutBlock(jpeg_buf->buf + jpeg_buf->idx - remain_bytes, bank_start_addr, &jpeg_asm_encode_context);
#if 0
    static jpeg_buf_t jpeg_buf_check;
    int old_idx;
    static int error_num = 0;
    memcpy(&jpeg_buf_check, jpeg_buf, sizeof(jpeg_buf_t));
    jpeg_buf_check.buf = jpeg_out_check_buf;
    old_idx = jpeg_buf_check.idx;
    for (row = 0; row < row_num;)
    {
        /* Each block is 4 rows and 2 cols */
        for (col = 0; col < col_num && block < len_block; col += 6, block++)
        {
            DCY = jpeg_encode_one_block(&jpeg_buf_check, (uint8_t *) & ((int16_t *)bank_addr)[2 * col], ch_size, fdtbl_Y, DCY, YDC_HT, YAC_HT);
            DCU = jpeg_encode_one_block(&jpeg_buf_check, (uint8_t *) & ((int16_t *)bank_addr)[2 * (col + 2)], ch_size, fdtbl_UV, DCU, UVDC_HT, UVAC_HT);
            DCV = jpeg_encode_one_block(&jpeg_buf_check, (uint8_t *) & ((int16_t *)bank_addr)[2 * (col + 4)], ch_size, fdtbl_UV, DCV, UVDC_HT, UVAC_HT);
        }

        row += 4;

        if ((row & 0x7) == 0)
        {
            bank_addr += 0x8000;
            bank_addr -= 0x10000;
            bank_addr += 4 * col_num;
        }
        else
        {
            bank_addr += 0x8000;
        }

    }
    for (i = jpeg_buf->idx; i < cur_buf_addr - jpeg_buf->buf; i++)
    {
        if (jpeg_buf->buf[i] != jpeg_buf_check.buf[i])
        {
            error_num++;
        }
    }
#endif
    jpeg_buf->bitc = 32 - jpeg_asm_encode_context.bitc;
    jpeg_buf->bitb = jpeg_asm_encode_context.bitb;
    jpeg_buf->idx += cur_buf_addr - (jpeg_buf->buf + jpeg_buf->idx);

    /* Put remain bytes into buffer */
    while (jpeg_buf->bitc > 7)
    {
        uint8_t c = (jpeg_buf->bitb >> 24) & 255;
        jpeg_put_char(jpeg_buf, c);
        if (c == 255)
        {
            jpeg_put_char(jpeg_buf, 0);
        }
        jpeg_buf->bitb <<= 8;
        jpeg_buf->bitc -= 8;
    }
    jpeg_buf->bitb = (jpeg_buf->bitb >> 8);

#else
    for (row = 0; row < row_num;)
    {
        /* Each block is 4 rows and 2 cols */
        for (col = 0; col < col_num && block < len_block; col += 6, block++)
        {
            DCY = jpeg_encode_one_block(jpeg_buf, (uint8_t *) & ((int16_t *)bank_addr)[2 * col], ch_size, fdtbl_Y, DCY, YDC_HT, YAC_HT);
            DCU = jpeg_encode_one_block(jpeg_buf, (uint8_t *) & ((int16_t *)bank_addr)[2 * (col + 2)], ch_size, fdtbl_UV, DCU, UVDC_HT, UVAC_HT);
            DCV = jpeg_encode_one_block(jpeg_buf, (uint8_t *) & ((int16_t *)bank_addr)[2 * (col + 4)], ch_size, fdtbl_UV, DCV, UVDC_HT, UVAC_HT);
        }

        row += 4;

        if ((row & 0x7) == 0)
        {
            bank_addr += 0x8000;
            bank_addr -= 0x10000;
            bank_addr += 4 * col_num;
        }
        else
        {
            bank_addr += 0x8000;
        }

    }
#endif
}


static void jpeg_do_hw_dct(struct npu_hw_config *cxt, int32_t isAB, int32_t row_num, int32_t col_num, int32_t ch_num, int32_t out_row, int32_t out_col, int32_t out_ch)
{
    /* Set start address for all commands and all weights */
    cxt->cmd_addr = cxt->cmd_start_addr = cxt->cmd_seq_start_addr = (void *)conv_cmd;
    cxt->weight_addr = cxt->weight_start_addr = cxt->weight_layer_start_addr = cxt->weight_seq_start_addr = cxt->weight_seq_end_addr = (void *)conv_weight;

    cxt->row_num = row_num;
    cxt->col_num = col_num;
    cxt->ch_num  = ch_num;
    cxt->out_row = out_row;
    cxt->out_col = out_col;
    cxt->out_ch  = out_ch;
    /* only support even channel */
    if ((cxt->ch_num & 0x1))
    {
        cxt->ch_num++;
    }

    /* We always make rows as multiples of 8 */
    cxt->clear_up = 0;
    cxt->clear_left = 0;
    cxt->clear_right = 0;
    cxt->clear_bottom = 0;

    cxt->pad_up = 0;
    cxt->pad_left = 0;
    cxt->pad_right = 0;
    cxt->pad_bottom = 0;
    cxt->pool_pad_up = 0;

    /* Inout image is not in bank first of all */
    cxt->isAB = isAB;

    cxt->nop_after_conv = 15;
    cxt->relu_en     = 0;
    cxt->relu_num    = 0;

    /* No meaning now, fixed to 1 */
    cxt->bank_sel_spad = 1;
    cxt->filter_size = 3;
    cxt->pool_size  = 1;
    cxt->pool_mode   = 0;
    cxt->conv_stride = 2;
    cxt->cnn_finish = 1;

    /* Generate conv command */
    npu_cmd_gen(cxt);

    /* Execute conv command */
    npu_cmd_exec(cxt);
}

void jpeg_encode_blocks(struct npu_hw_config *cxt, uint8_t *src, int32_t bpp, int32_t width, int32_t height, jpeg_buf_t *jpeg_buf)
{
    int32_t start_block, len_block, max_blocks, total_blocks;
    int32_t ch_num, row_num, col_num, bank_max_row, spad_max_row;
    int32_t image_ch;

    image_ch = 3;
    /* Caculate col_num and row_num */
    /* Decrease col_num(by increasing row_num) if col_num is much more than row_num */
    /* Each block is 2 chs, 8 rows and 4 cols */
    ch_num = 2;
    col_num = (255 / (4 * image_ch)) * (4 * image_ch);
    bank_max_row = SRAM_SIZE / (col_num * ch_num * 2);
    spad_max_row = SPAD_SIZE / (col_num * 2);
    row_num = (bank_max_row < spad_max_row) ?  bank_max_row : spad_max_row;
    row_num = (row_num / 8) * 8;
    max_blocks = (row_num / 8) * (col_num / (4 * image_ch));
    total_blocks = ((width + 7) / 8) * ((height + 7) / 8);

    for (start_block = 0, len_block = 0; start_block < total_blocks; start_block += len_block)
    {
        len_block = total_blocks - start_block;
        if (len_block >= max_blocks)
        {
            len_block = max_blocks;
        }
        row_num = DIVCEIL(max_blocks, (col_num / (4 * image_ch))) * 8;
        /* Get image blocks from memory to bank */
        image_get_blocks(src, cxt->bank_a_addr, image_ch, start_block, len_block, width, height, ch_num, row_num, col_num);

        //check_get_blocks(src, cxt->bank_a_addr, image_ch, start_block, len_block, width, height, ch_num, row_num, col_num);

        /* Calculate 8x8 dct */
        jpeg_do_hw_dct(cxt, 1, row_num, col_num, 2, row_num / 2, col_num / 2, 8);
#ifdef ARMC4_SIMULATOR
        copy_row_dct(src, cxt->bank_c_addr, image_ch, start_block, len_block, width, height, 8, row_num / 2, col_num / 2);
#endif
        //check_row_dct(src, cxt->bank_c_addr, image_ch, start_block, len_block, width, height, 8, row_num/2, col_num/2);
        dct_bank_channel_merge(cxt->bank_c_addr, cxt->bank_a_addr, row_num / 2, col_num / 2);
        jpeg_do_hw_dct(cxt, 1, row_num, col_num, 2, row_num / 2, col_num / 2, 8);
#ifdef ARMC4_SIMULATOR
        copy_col_dct(src, cxt->bank_c_addr, image_ch, start_block, len_block, width, height, 8, row_num / 2, col_num / 2);
#endif
        //check_col_dct(src, cxt->bank_c_addr, image_ch, start_block, len_block, width, height, 8, row_num/2, col_num/2);
        /* Put dct macro blocks from bank to stream, and do stream coding */
        jpeg_put_blocks(jpeg_buf, cxt->bank_c_addr, len_block, 8, row_num / 2, col_num / 2);
    }
}

static uint32_t *convert_dct_one_channel(uint32_t *output, int32_t idx)
{
    int32_t  convert_w0, convert_w1, convert_w2;
    uint32_t convert_w;

    /* Convert gamma/betta/bias/0 */
    *(output++) = (0x0000 << 16) + (0x0100 & 0xFFFF);
    *(output++) = (0x0000 & 0xFFFF);

    /*
     *  A00 A01 0   A04 A05 0   0 0 0   0 0 0        z-z-z  z-z-z
     *  A02 A03 0   A06 A07 0   0 0 0   0 0 0  --->  z-3-1  z-7-5
     *    0   0 0     0   0 0   0 0 0   0 0 0        z-2-0  z-6-4
     */
    *(output++) = 0;
    convert_w0 = 0;
    convert_w1 = dct_fix_matrix[8 * idx + 3];
    convert_w2 = dct_fix_matrix[8 * idx + 1];
    convert_w = (((uint32_t)convert_w0) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w1) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w2) & 0x3FF);
    *(output++) = convert_w;
    convert_w0 = 0;

    convert_w1 = dct_fix_matrix[8 * idx + 2];
    convert_w2 = dct_fix_matrix[8 * idx + 0];
    convert_w = (((uint32_t)convert_w0) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w1) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w2) & 0x3FF);
    *(output++) = convert_w;

    *(output++) = 0;
    convert_w0 = 0;
    convert_w1 = dct_fix_matrix[8 * idx + 7];
    convert_w2 = dct_fix_matrix[8 * idx + 5];
    convert_w = (((uint32_t)convert_w0) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w1) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w2) & 0x3FF);
    *(output++) = convert_w;
    convert_w0 = 0;
    convert_w1 = dct_fix_matrix[8 * idx + 6];
    convert_w2 = dct_fix_matrix[8 * idx + 4];
    convert_w = (((uint32_t)convert_w0) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w1) & 0x3FF);
    convert_w = (convert_w << 10) + (((uint32_t)convert_w2) & 0x3FF);
    *(output++) = convert_w;

    return output;
}

int jpeg_hal_init(void)
{
    int i;
    uint32_t *weight_addr = (uint32_t *)conv_weight;

    /* Convert dct row weight (stage 0) , 2 input channel, 8 output channel */
    for (i = 0; i < 8; i++)
    {
        weight_addr = convert_dct_one_channel(weight_addr, i);
    }

    /* Convert dct col weight (stage 1) , 2 input channel, 8 output channel */
    for (i = 0; i < 8; i++)
    {
        weight_addr = convert_dct_one_channel(weight_addr, i);
    }

    for (i = 0; i < 64; i++)
    {
        r_jpeg_ZigZag[s_jpeg_ZigZag[i]] = i;
    }

    npu_dev_init(&jpeg_npu_config);
#ifdef YUV_ASM_TAB
    extern int8_t yuv_table[];
    int r, g, b;
    uint16_t rgb565;

    for (r = 133; r < 256; r++)
    {
        for (g = 133; g < 256; g++)
        {
            for (b = 133; b < 256; b++)
            {
                rgb565 = (rb825_table[r] << 11) + (g826_table[g] << 5) + (rb825_table[b] << 0);
                yuv_table[4 * rgb565] = fast_roundf(+0.29900f * r + 0.58700f * g + 0.11400f * b - 128);
                yuv_table[4 * rgb565 + 1] = fast_roundf(-0.16874f * r - 0.33126f * g + 0.50000f * b);
                yuv_table[4 * rgb565 + 2] = fast_roundf(+0.50000f * r - 0.41869f * g - 0.08131f * b);
                yuv_table[4 * rgb565 + 3] = 0;
#if 0
                rgb565 = ((uint16_t)(r << 11)) + ((uint16_t)(g << 5)) + ((uint16_t)(b << 0));
                yuv_table[4 * rgb565] = fast_roundf(+0.29900f * r * 8 + 0.58700f * g * 4 + 0.11400f * b * 8 - 128);
                yuv_table[4 * rgb565 + 1] = fast_roundf(-0.16874f * r * 8 - 0.33126f * g * 4 + 0.50000f * b * 8);
                yuv_table[4 * rgb565 + 2] = fast_roundf(+0.50000f * r * 8 - 0.41869f * g * 4 - 0.08131f * b * 8);
                yuv_table[4 * rgb565 + 3] = 0;
#endif
            }
        }
    }
#endif
    /* Generate dct cmd */
    return 0;
}


int jpeg_hal_compress(image_t *src, image_t *dst, int quality)
{
    // JPEG buffer
    jpeg_buf_t  jpeg_buf =
    {
        .idx = 0,
        .buf = dst->u.pixels,
        .length = dst->bpp,
        .bitc = 0,
        .bitb = 0,
        .realloc = 0,
        .overflow = FALSE,
    };

    jpeg_subsample_t jpeg_subsample;

    if (quality >= 60)
    {
        jpeg_subsample = JPEG_SUBSAMPLE_1x1;
    }
    else if (quality > 35)
    {
        jpeg_subsample = JPEG_SUBSAMPLE_2x1;
    }
    else     // <= 35
    {
        jpeg_subsample = JPEG_SUBSAMPLE_2x2;
    }

    /* Initialize quantization tables */
    DCY = 0;
    DCU = 0;
    DCV = 0;
    jpeg_asm_encode_context.DC[0] = 0;
    jpeg_asm_encode_context.DC[1] = 0;
    jpeg_asm_encode_context.DC[2] = 0;
    jpeg_init(quality);

    jpeg_write_headers(&jpeg_buf, src->w, src->h, src->bpp, jpeg_subsample);

    jpeg_encode_blocks(&jpeg_npu_config, src->u.data, 3, src->w, src->h, &jpeg_buf);

    // Do the bit alignment of the EOI marker
    static const uint16_t fillBits[] = {0x7F, 7};
    jpeg_writeBits(&jpeg_buf, fillBits);

    // EOI
    jpeg_put_char(&jpeg_buf, 0xFF);
    jpeg_put_char(&jpeg_buf, 0xD9);

    dst->bpp = jpeg_buf.idx;
    dst->u.data = jpeg_buf.buf;

    //printf("dst->bpp = 0x%x\n",dst->bpp);

    return 0;
}

#define JPEG_DEBUG
#ifdef JPEG_DEBUG
static int jpeg_processDU(jpeg_buf_t *jpeg_buf, int8_t *CDU, float *fdtbl, int DC, const uint16_t (*HTDC)[2], const uint16_t (*HTAC)[2])
{
    int DU[64];
    int DUQ[64];
    int z1, z2, z3, z4, z5, z11, z13;
    int t0, t1, t2, t3, t4, t5, t6, t7, t10, t11, t12, t13;
    const uint16_t EOB[2] = { HTAC[0x00][0], HTAC[0x00][1] };
    const uint16_t M16zeroes[2] = { HTAC[0xF0][0], HTAC[0xF0][1] };

    // DCT rows
    for (int i = 8, *p = DU; i > 0; i--, p += 8, CDU += 8)
    {
        t0 = CDU[0] + CDU[7];
        t1 = CDU[1] + CDU[6];
        t2 = CDU[2] + CDU[5];
        t3 = CDU[3] + CDU[4];

        t7 = CDU[0] - CDU[7];
        t6 = CDU[1] - CDU[6];
        t5 = CDU[2] - CDU[5];
        t4 = CDU[3] - CDU[4];
#if 0
        // Even part
        t10 = t0 + t3;
        t13 = t0 - t3;
        t11 = t1 + t2;
        t12 = t1 - t2;
        z1  = LMULTIPLY(t12 + t13, FIX_0_707106781); // c4

        p[0] = t10 + t11;
        p[4] = t10 - t11;
        p[2] = t13 + z1;
        p[6] = t13 - z1;

        // Odd part
        t10 = t4 + t5;// phase 2
        t11 = t5 + t6;
        t12 = t6 + t7;

        // The rotator is modified from fig 4-8 to avoid extra negations.
        z5 = LMULTIPLY(t10 - t12, FIX_0_382683433); // c6
        z2 = LMULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
        z4 = LMULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
        z3 = LMULTIPLY(t11, FIX_0_707106781); // c4
        z11 = t7 + z3;    // phase 5
        z13 = t7 - z3;

        p[5] = z13 + z2;// phase 6
        p[3] = z13 - z2;
        p[1] = z11 + z4;
        p[7] = z11 - z4;
#else
        // Even part
        t10 = t0 + t3;
        t13 = t0 - t3;
        t11 = t1 + t2;
        t12 = t1 - t2;
        z1  = MULTIPLY(t12 + t13, FIX_0_707106781);

        p[0] = t10 + t11;
        p[4] = t10 - t11;
        p[2] = DESCALE(t13 * FIX_1_0000000 + z1, 8);
        p[6] = DESCALE(t13 * FIX_1_0000000 - z1, 8);

        // Odd part
        t10 = t4 + t5;// phase 2
        t11 = t5 + t6;
        t12 = t6 + t7;


        z5 = MULTIPLY(t10 - t12, FIX_0_382683433); // c6
        z2 = MULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
        z4 = MULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
        z3 = MULTIPLY(t11, FIX_0_707106781); // c4
        z11 = t7 * FIX_1_0000000 + z3;  // phase 5
        z13 = t7 * FIX_1_0000000 - z3;

        p[5] = DESCALE(z13 + z2, 8);// phase 6
        p[3] = DESCALE(z13 - z2, 8);
        p[1] = DESCALE(z11 + z4, 8);
        p[7] = DESCALE(z11 - z4, 8);
#endif
    }

    // DCT columns
    for (int i = 8, *p = DU; i > 0; i--, p++)
    {
        t0 = p[0]  + p[56];
        t1 = p[8]  + p[48];
        t2 = p[16] + p[40];
        t3 = p[24] + p[32];

        t7 = p[0]  - p[56];
        t6 = p[8]  - p[48];
        t5 = p[16] - p[40];
        t4 = p[24] - p[32];
#if 0
        // Even part
        t10 = t0 + t3;  // phase 2
        t13 = t0 - t3;
        t11 = t1 + t2;
        t12 = t1 - t2;
        z1 = LMULTIPLY(t12 + t13, FIX_0_707106781); // c4

        p[0] = t10 + t11;       // phase 3
        p[32] = t10 - t11;
        p[16] = t13 + z1;       // phase 5
        p[48] = t13 - z1;

        // Odd part
        t10 = t4 + t5;      // phase 2
        t11 = t5 + t6;
        t12 = t6 + t7;

        // The rotator is modified from fig 4-8 to avoid extra negations.
        z5 = LMULTIPLY(t10 - t12, FIX_0_382683433); // c6
        z2 = LMULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
        z4 = LMULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
        z3 = LMULTIPLY(t11, FIX_0_707106781); // c4
        z11 = t7 + z3;      // phase 5
        z13 = t7 - z3;

        p[40] = z13 + z2;// phase 6
        p[24] = z13 - z2;
        p[8] = z11 + z4;
        p[56] = z11 - z4;
#else
        // Even part
        t10 = t0 + t3;  // phase 2
        t13 = t0 - t3;
        t11 = t1 + t2;
        t12 = t1 - t2;
        z1 = MULTIPLY(t12 + t13, FIX_0_707106781); // c4

        p[0]  = t10 + t11;      // phase 3
        p[32] = t10 - t11;
        p[16] = DESCALE(t13 * FIX_1_0000000 + z1, 8);       // phase 5
        p[48] = DESCALE(t13 * FIX_1_0000000 - z1, 8);

        // Odd part
        t10 = t4 + t5;      // phase 2
        t11 = t5 + t6;
        t12 = t6 + t7;

        z5 = MULTIPLY(t10 - t12, FIX_0_382683433); // c6
        z2 = MULTIPLY(t10, FIX_0_541196100) + z5; // 1.306562965f-c6
        z4 = MULTIPLY(t12, FIX_1_306562965) + z5; // 1.306562965f+c6
        z3 = MULTIPLY(t11, FIX_0_707106781); // c4
        z11 = t7 * FIX_1_0000000 + z3;      // phase 5
        z13 = t7 * FIX_1_0000000 - z3;

        p[40] = DESCALE(z13 + z2, 8);// phase 6
        p[24] = DESCALE(z13 - z2, 8);
        p[8]  = DESCALE(z11 + z4, 8);
        p[56] = DESCALE(z11 - z4, 8);
#endif
    }


    // first non-zero element in reverse order
    int end0pos = 0;
    // Quantize/descale/zigzag the coefficients
    for (int i = 0; i < 64; ++i)
    {
        DUQ[s_jpeg_ZigZag[i]] = fast_roundf(DU[i] * fdtbl[i]);
        if (s_jpeg_ZigZag[i] > end0pos && DUQ[s_jpeg_ZigZag[i]])
        {
            end0pos = s_jpeg_ZigZag[i];
        }
    }

    // Encode DC
    int diff = DUQ[0] - DC;
    if (diff == 0)
    {
        jpeg_writeBits(jpeg_buf, HTDC[0]);
    }
    else
    {
        uint16_t bits[2];
        jpeg_calcBits(diff, bits);
        jpeg_writeBits(jpeg_buf, HTDC[bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }

    // Encode ACs
    if (end0pos == 0)
    {
        jpeg_writeBits(jpeg_buf, EOB);
        return DUQ[0];
    }

    for (int i = 1; i <= end0pos; ++i)
    {
        int startpos = i;
        for (; DUQ[i] == 0 && i <= end0pos ; ++i)
        {
        }
        int nrzeroes = i - startpos;
        if (nrzeroes >= 16)
        {
            int lng = nrzeroes >> 4;
            for (int nrmarker = 1; nrmarker <= lng; ++nrmarker)
            {
                jpeg_writeBits(jpeg_buf, M16zeroes);
            }
            nrzeroes &= 15;
        }
        uint16_t bits[2];
        jpeg_calcBits(DUQ[i], bits);
        jpeg_writeBits(jpeg_buf, HTAC[(nrzeroes << 4) + bits[1]]);
        jpeg_writeBits(jpeg_buf, bits);
    }
    if (end0pos != 63)
    {
        jpeg_writeBits(jpeg_buf, EOB);
    }
    return DUQ[0];
}

int jpeg_compress(image_t *src, image_t *dst, int quality)
{
    // JPEG buffer
    jpeg_buf_t  jpeg_buf =
    {
        .idx = 0,
        .buf = dst->u.pixels,
        .length = dst->bpp,
        .bitc = 0,
        .bitb = 0,
        .realloc = 0,
        .overflow = FALSE,
    };

    // Initialize quantization tables
    jpeg_init(quality);

    jpeg_subsample_t jpeg_subsample;

    if (quality >= 60)
    {
        jpeg_subsample = JPEG_SUBSAMPLE_1x1;
    }
    else if (quality > 35)
    {
        jpeg_subsample = JPEG_SUBSAMPLE_2x1;
    }
    else     // <= 35
    {
        jpeg_subsample = JPEG_SUBSAMPLE_2x2;
    }

    int i = 0;
    int row = 0;
    int col = 0;
    int k = 0;
    unsigned int end, start = 0;
    unsigned int time;


    const void *data = src->u.data;
    int width = src->w;
    int height = src->h;
    int comp = 3;

    if (!data || !width || !height || comp > 4 || comp < 1 || comp == 2)
    {
        return 0;
    }

    jpeg_write_headers(&jpeg_buf, src->w, src->h, (src->bpp == 0) ? 1 : src->bpp, jpeg_subsample);

    // Encode 8x8 macroblocks
    const unsigned char *imageData = (const unsigned char *)data;
    int DCY = 0, DCU = 0, DCV = 0;
    int bitBuf = 0, bitCnt = 0;
    int ofsG = comp > 1 ? 1 : 0, ofsB = comp > 1 ? 2 : 0;
    for (int y = 0; y < height; y += 8)
    {
        for (int x = 0; x < width; x += 8)
        {
            signed char YDU[64], UDU[64], VDU[64];
            for (int row = y, pos = 0; row < y + 8; ++row)
            {
                for (int col = x; col < x + 8; ++col, ++pos)
                {
                    int p = row * width * comp + col * comp;
                    if (row >= height)
                    {
                        p -= width * comp * (row + 1 - height);
                    }
                    if (col >= width)
                    {
                        p -= comp * (col + 1 - width);
                    }

                    unsigned char b = imageData[p + 0], g = imageData[p + ofsG], r = imageData[p + ofsB];

                    YDU[pos] = (signed char)(+0.29900f * r + 0.58700f * g + 0.11400f * b - 128);
                    UDU[pos] = (signed char)(-0.16874f * r - 0.33126f * g + 0.50000f * b);
                    VDU[pos] = (signed char)(+0.50000f * r - 0.41869f * g - 0.08131f * b);
                }
            }

            DCY = jpeg_processDU(&jpeg_buf,  YDU, fdtbl_Y, DCY, YDC_HT, YAC_HT);
            DCU = jpeg_processDU(&jpeg_buf, UDU, fdtbl_UV, DCU, UVDC_HT, UVAC_HT);
            DCV = jpeg_processDU(&jpeg_buf,  VDU, fdtbl_UV, DCV, UVDC_HT, UVAC_HT);
        }
    }

    // Do the bit alignment of the EOI marker
    static const unsigned short fillBits[] = {0x7F, 7};
    jpeg_writeBits(&jpeg_buf, fillBits);

    // EOI
    jpeg_put_char(&jpeg_buf, 0xFF);
    jpeg_put_char(&jpeg_buf, 0xD9);

    dst->bpp = jpeg_buf.idx;
    dst->u.data = jpeg_buf.buf;

    return jpeg_buf.idx;

}
#endif




