[net]
width=300
height=300
channels=3

# conv0
[convolutional]
out_ch=32
kernel_size=3
stride=2
# padding should be 'same', but if set pad=1, easynet will add zero pading, 
# actually it does not need zero padding
pad=0
[batchnorm]
[activation]
activation=relu

# conv1
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=64
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv2
[depthwise_convolutional]
kernel_size=3
stride=2
# padding should be 'same', but if set pad=1, easynet will add zero pading, 
# actually it does not need zero padding
pad=0
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=128
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv3
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=128
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv4
[depthwise_convolutional]
kernel_size=3
stride=2
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=256
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv5
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=256
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv6
[depthwise_convolutional]
kernel_size=3
stride=2
# padding should be 'same', but if set pad=1, easynet will add zero pading, 
# actually it does not need zero padding
pad=0
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv7
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv8
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv9
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv10
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv11
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv12
[depthwise_convolutional]
kernel_size=3
stride=2
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=1024
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv13
[depthwise_convolutional]
kernel_size=3
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=1024
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu

# conv14
[convolutional]
out_ch=256
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=512
kernel_size=3
stride=2
# padding should be 'same', but if set pad=1, easynet will add zero pading, 
# actually it does not need zero padding
pad=0
[batchnorm]
[activation]
activation=relu

# conv15
[convolutional]
out_ch=128
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=256
kernel_size=3
stride=2
pad=1
[batchnorm]
[activation]
activation=relu

# conv16
[convolutional]
out_ch=128
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=256
kernel_size=3
stride=2
pad=1
[batchnorm]
[activation]
activation=relu

# conv17
[convolutional]
out_ch=64
kernel_size=1
stride=1
pad=1
[batchnorm]
[activation]
activation=relu
[convolutional]
out_ch=128
kernel_size=3
stride=2
# padding should be 'same', but if set pad=1, easynet will add zero pading, 
# actually it does not need zero padding
pad=0
[batchnorm]
[activation]
activation=relu





